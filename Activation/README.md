# LLaMA Attention Head Activation Collection

ç”¨äºåœ¨ LLaMA 3.2-1B æ¨¡å‹ä¸Šé‡‡é›†æ¯å±‚æ¯ä¸ª attention head çš„æ¿€æ´»å¼ºåº¦ä¿¡å·çš„ Python é¡¹ç›®ã€‚

## é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®å®ç°äº†åœ¨ decoder-only æ¨¡å‹ï¼ˆLLaMA 3.2-1Bï¼‰ä¸Šé‡‡é›†ä¸¤ç±» attention head å¼ºåº¦ä¿¡å·ï¼š

1. **Head Output Norm**: æ¯ä¸ª attention head è¾“å‡ºçš„ L2 èŒƒæ•°ï¼ˆåˆå¹¶å‰ï¼‰
2. **Head Residual Contribution Norm**: æ¯ä¸ª head ç»è¿‡è¾“å‡ºæŠ•å½±å±‚åå¯¹æ®‹å·®æµçš„è´¡çŒ®çš„ L2 èŒƒæ•°

### ä¸»è¦ç‰¹æ€§

- âœ… æ”¯æŒ LLaMA 3.2-1Bï¼ˆtransformers 4.5xï¼‰
- âœ… æ”¯æŒ ARC-Challenge æ•°æ®é›†ï¼ˆ4-5 ä¸ªé€‰é¡¹ï¼ŒA-Eï¼‰
- âœ… çµæ´»çš„ Prompt æ¨¡æ¿ç³»ç»Ÿï¼ˆé€‚é… SFT/LoRA å¾®è°ƒï¼‰
- âœ… åœ¨çº¿ç»Ÿè®¡ï¼ˆWelford ç®—æ³•ï¼‰ï¼Œé¿å…å†…å­˜æº¢å‡º
- âœ… æ”¯æŒä¸¤ç§ token èšåˆç­–ç•¥ï¼šlast / all
- âœ… è‡ªåŠ¨ç”Ÿæˆçƒ­åŠ›å›¾å¯è§†åŒ–
- âœ… å¯æ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†

## é¡¹ç›®ç»“æ„

```
project/
â”œâ”€â”€ README.md                  # æœ¬æ–‡æ¡£
â”œâ”€â”€ requirements.txt           # Python ä¾èµ–
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml          # é»˜è®¤é…ç½®æ–‡ä»¶
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py               # ä¸»ç¨‹åºå…¥å£
â”‚   â”œâ”€â”€ config.py             # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ data/                 # æ•°æ®å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ dataset_base.py   # æ•°æ®é›†åŸºç±»
â”‚   â”‚   â”œâ”€â”€ arc.py            # ARC-Challenge æ•°æ®é›†
â”‚   â”‚   â””â”€â”€ prompt.py         # Prompt æ¨¡æ¿æ„å»ºå™¨
â”‚   â”œâ”€â”€ model/                # æ¨¡å‹å±‚
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ loader.py         # æ¨¡å‹åŠ è½½
â”‚   â”‚   â”œâ”€â”€ hooks.py          # Hook ç®¡ç†å™¨ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”‚   â””â”€â”€ metrics.py        # åœ¨çº¿ç»Ÿè®¡
â”‚   â””â”€â”€ utils/                # å·¥å…·å‡½æ•°
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ seed.py           # éšæœºç§å­
â”‚       â”œâ”€â”€ io.py             # æ–‡ä»¶ I/O
â”‚       â””â”€â”€ logging.py        # æ—¥å¿—
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_arc_collect.sh    # è¿è¡Œè„šæœ¬
â””â”€â”€ outputs/                  # è¾“å‡ºç›®å½•ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
```

## ç¯å¢ƒå®‰è£…

### ç³»ç»Ÿè¦æ±‚

- Python >= 3.8
- CUDA >= 11.8ï¼ˆæ¨èï¼‰
- GPU å†…å­˜ >= 8GBï¼ˆLLaMA 3.2-1B bf16ï¼‰

### å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

ä¸»è¦ä¾èµ–ï¼š

```
torch>=2.0.0
transformers>=4.50.0,<4.60.0
datasets>=2.14.0
numpy>=1.24.0
pyyaml>=6.0
tqdm>=4.65.0
matplotlib>=3.7.0
```

## æ•°æ®å‡†å¤‡

### ARC-Challenge æ•°æ®é›†

æœ¬é¡¹ç›®æ”¯æŒä¸¤ç§æ•°æ®åŠ è½½æ–¹å¼ï¼š

#### æ–¹å¼ 1: æœ¬åœ° JSONL æ–‡ä»¶ï¼ˆæ¨èï¼‰

å°† ARC-Challenge æ•°æ®ä¿å­˜ä¸º JSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼š

```jsonl
{"id": "Mercury_7220990", "question": "Which property of a mineral...", "choices": {"text": ["color", "hardness", "luster", "streak"], "label": ["A", "B", "C", "D"]}, "answerKey": "D"}
```

**å­—æ®µè¯´æ˜**ï¼š

- `id`: é—®é¢˜ ID
- `question`: é—®é¢˜æ–‡æœ¬
- `choices`: é€‰é¡¹å­—å…¸
  - `text`: é€‰é¡¹æ–‡æœ¬åˆ—è¡¨
  - `label`: é€‰é¡¹æ ‡ç­¾åˆ—è¡¨ï¼ˆå¯ä»¥æ˜¯å­—æ¯æˆ–æ•°å­—ï¼‰
- `answerKey`: æ­£ç¡®ç­”æ¡ˆï¼ˆå­—æ¯ A/B/C/D/E æˆ–æ•°å­— 1/2/3/4/5ï¼‰

æ–‡ä»¶å‘½åï¼š`test.jsonl` æˆ– `ARC-Challenge-test.jsonl`

#### æ–¹å¼ 2: HuggingFace Datasets ç¼“å­˜

```python
from datasets import load_dataset

# ä¸‹è½½å¹¶ç¼“å­˜æ•°æ®é›†
dataset = load_dataset("ai2_arc", "ARC-Challenge", cache_dir="/data/datasets/arc_challenge/")
```

ç„¶åå°† `data_dir` è®¾ç½®ä¸ºç¼“å­˜ç›®å½•ã€‚

### é€‰é¡¹æ•°é‡æ”¯æŒ

- âœ… **4 ä¸ªé€‰é¡¹**ï¼ˆA/B/C/Dï¼‰
- âœ… **5 ä¸ªé€‰é¡¹**ï¼ˆA/B/C/D/Eï¼‰
- âŒ å…¶ä»–æ•°é‡çš„é€‰é¡¹ä¼šè¢«è·³è¿‡å¹¶è®°å½•æ—¥å¿—

### Answer Key æ˜ å°„è§„åˆ™

æœ¬é¡¹ç›®è‡ªåŠ¨å¤„ç†ä¸åŒæ ¼å¼çš„ `answerKey`ï¼š

- **å­—æ¯æ ¼å¼**: "A", "B", "C", "D", "E" â†’ ç›´æ¥ä½¿ç”¨
- **æ•°å­—æ ¼å¼**: "1", "2", "3", "4", "5" â†’ æ˜ å°„åˆ° A/B/C/D/Eï¼ˆ1-basedï¼‰

## ä½¿ç”¨æ–¹æ³•

### å¿«é€Ÿå¼€å§‹

1. **ä¿®æ”¹é…ç½®æ–‡ä»¶** `configs/default.yaml`ï¼š

```yaml
model_path: "/data/models/llama-3.2-1b/"    # æ¨¡å‹è·¯å¾„
data_dir: "/data/datasets/arc_challenge/"    # æ•°æ®è·¯å¾„
output_dir: "./outputs"                       # è¾“å‡ºè·¯å¾„
max_samples: 5000                             # æœ€å¤§æ ·æœ¬æ•°ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨ï¼‰
batch_size: 4                                 # æ‰¹å¤§å°
max_length: 384                               # æœ€å¤§åºåˆ—é•¿åº¦
dtype: "bf16"                                 # bf16/fp16/fp32
device_map: "auto"                            # auto/cuda/cpu
token_agg: "last"                             # last/all
template_name: "arc_mcq_v1"                   # Prompt æ¨¡æ¿
seed: 42                                      # éšæœºç§å­
```

2. **è¿è¡Œè„šæœ¬**ï¼š

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶
python -m src.main --config configs/default.yaml

# æˆ–ä½¿ç”¨ bash è„šæœ¬
bash scripts/run_arc_collect.sh
```

3. **å‘½ä»¤è¡Œå‚æ•°è¦†ç›–**ï¼ˆå¯é€‰ï¼‰ï¼š

```bash
python -m src.main \
    --config configs/default.yaml \
    --model_path /data/models/llama-3.2-1b/ \
    --data_dir /data/datasets/arc_challenge/ \
    --max_samples 1000 \
    --batch_size 8
```

### Token èšåˆç­–ç•¥

- **`last`** (é»˜è®¤): æ¯ä¸ªæ ·æœ¬å–æœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„æ¿€æ´»
- **`all`**: å¯¹æ‰€æœ‰æœ‰æ•ˆ token å–å¹³å‡ï¼ˆæ’é™¤ paddingï¼‰

### è¾“å‡ºæ–‡ä»¶

è¿è¡Œå®Œæˆåï¼Œåœ¨ `outputs/<experiment_name>_<timestamp>/` ç›®å½•ä¸‹ç”Ÿæˆï¼š

```
outputs/arc_head_activation_20250106_123456/
â”œâ”€â”€ config.json                              # è¿è¡Œé…ç½®
â”œâ”€â”€ meta.json                                # å…ƒæ•°æ®ï¼ˆæ¨¡å‹ä¿¡æ¯ã€ç»Ÿè®¡æ•°æ®ç­‰ï¼‰
â”œâ”€â”€ head_output_norm_mean.npy                # Head Output Norm å‡å€¼ [num_layers, num_heads]
â”œâ”€â”€ head_output_norm_std.npy                 # Head Output Norm æ ‡å‡†å·®
â”œâ”€â”€ head_resid_contrib_norm_mean.npy         # Head Residual Contribution Norm å‡å€¼
â”œâ”€â”€ head_resid_contrib_norm_std.npy          # æ ‡å‡†å·®
â”œâ”€â”€ head_output_norm_heatmap.png             # çƒ­åŠ›å›¾ï¼ˆHead Output Normï¼‰
â””â”€â”€ head_resid_contrib_norm_heatmap.png      # çƒ­åŠ›å›¾ï¼ˆHead Residual Contribution Normï¼‰
```

### è¯»å–ç»“æœ

```python
import numpy as np
import json

# åŠ è½½æ¿€æ´»æ•°æ®
head_output_norm = np.load("outputs/.../head_output_norm_mean.npy")
head_resid_norm = np.load("outputs/.../head_resid_contrib_norm_mean.npy")

# åŠ è½½å…ƒæ•°æ®
with open("outputs/.../meta.json", "r") as f:
    meta = json.load(f)

print(f"Shape: {head_output_norm.shape}")  # (num_layers, num_heads)
print(f"Processed samples: {meta['num_processed']}")
```

## Prompt æ¨¡æ¿

### arc_mcq_v1ï¼ˆé»˜è®¤ï¼‰

è®¾è®¡åŸåˆ™ï¼š
- âœ… æ¸…æ™°çš„ä»»åŠ¡æŒ‡ä»¤
- âœ… å¼ºåˆ¶å•å­—æ¯è¾“å‡ºï¼ˆA/B/C/D/Eï¼‰
- âœ… éšè—æ¨ç†è¿‡ç¨‹ï¼ˆè®­ç»ƒå‹å¥½ï¼‰
- âœ… é€‚é…å…¨é‡å¾®è°ƒå’Œ LoRA å¾®è°ƒ

ç¤ºä¾‹è¾“å‡ºï¼š

```
You are a careful reasoner. Read the question and choose the single best answer from the options.
Think step-by-step privately, but do not reveal your reasoning.
Return only the letter of the correct option (A, B, C, or D).

Question: Which property of a mineral can be determined just by looking at it?
Options:
A. color
B. hardness
C. luster
D. streak

Answer: 
```

### æ‰©å±•è‡ªå®šä¹‰æ¨¡æ¿

åœ¨ `src/data/prompt.py` ä¸­æ·»åŠ æ–°æ¨¡æ¿ï¼š

```python
@PromptBuilder.register_template("my_template")
def my_template(question, option_labels, option_texts, few_shot=0):
    # è‡ªå®šä¹‰ prompt æ„å»ºé€»è¾‘
    return prompt_text
```

## æŠ€æœ¯ç»†èŠ‚

### Hook æœºåˆ¶

æœ¬é¡¹ç›®é€šè¿‡ forward hook åœ¨æ¯ä¸ª attention å±‚é‡‡é›†æ¿€æ´»ï¼š

1. **Hook ä½ç½®**: `model.model.layers[i].self_attn`
2. **é‡‡é›†æ–¹å¼**: é‡è®¡ç®— attention ä»¥è·å– per-head è¾“å‡º
3. **å†…å­˜ä¼˜åŒ–**: ä½¿ç”¨åœ¨çº¿ç»Ÿè®¡ï¼ˆWelford ç®—æ³•ï¼‰ï¼Œä¸å­˜å‚¨æ‰€æœ‰æ¿€æ´»

### Head Output Norm

è®¡ç®—å…¬å¼ï¼š

```
å¯¹æ¯ä¸ª head hï¼š
  attn_output_h = attention_weights_h @ V_h   # [bs, seq_len, head_dim]
  norm_h = ||attn_output_h[token_pos]||_2     # L2 èŒƒæ•°
```

### Head Residual Contribution Norm

è®¡ç®—å…¬å¼ï¼š

```
å¯¹æ¯ä¸ª head hï¼š
  o_proj_slice = W_o[:, h*head_dim:(h+1)*head_dim]   # è¾“å‡ºæŠ•å½±çš„å¯¹åº”åˆ‡ç‰‡
  contribution_h = attn_output_h @ o_proj_slice^T    # [bs, seq_len, hidden_size]
  norm_h = ||contribution_h[token_pos]||_2           # L2 èŒƒæ•°
```

### æ€§èƒ½ä¼˜åŒ–

- ä»…åœ¨æŒ‡å®š token ä½ç½®è®¡ç®—ï¼ˆ`token_agg="last"`ï¼‰
- æ‰¹é‡çŸ©é˜µè¿ç®—
- åœ¨çº¿ç»Ÿè®¡é¿å…å­˜å‚¨å¤§å¼ é‡
- `use_cache=False` å‡å°‘å†…å­˜å ç”¨

## æ‰©å±•åˆ°å…¶ä»–æ•°æ®é›†

### æ­¥éª¤ 1: åˆ›å»ºæ•°æ®é›†ç±»

åœ¨ `src/data/` ä¸­åˆ›å»ºæ–°æ–‡ä»¶ï¼ˆå¦‚ `commonsenseqa.py`ï¼‰ï¼š

```python
from .dataset_base import DatasetBase

class CommonsenseQADataset(DatasetBase):
    def __init__(self, data_dir, template_name, ...):
        # å®ç°æ•°æ®åŠ è½½é€»è¾‘
        pass
    
    def __getitem__(self, idx):
        # è¿”å›æ ¼å¼åŒ–æ ·æœ¬
        return {
            "prompt_text": ...,
            "answer_letter": ...,
            "option_labels": ...,
            "target_text": ...,
            "meta": {...}
        }
```

### æ­¥éª¤ 2: æ³¨å†Œ Prompt æ¨¡æ¿

åœ¨ `src/data/prompt.py` ä¸­æ·»åŠ ï¼š

```python
@PromptBuilder.register_template("commonsenseqa_v1")
def commonsenseqa_template(question, option_labels, option_texts, few_shot=0):
    # æ„å»º prompt
    return prompt_text
```

### æ­¥éª¤ 3: ä¿®æ”¹é…ç½®

åœ¨ `configs/default.yaml` ä¸­ä¿®æ”¹ï¼š

```yaml
template_name: "commonsenseqa_v1"
```

## å…¼å®¹æ€§è¯´æ˜

### Transformers ç‰ˆæœ¬

- âœ… æ”¯æŒ transformers 4.50 - 4.59
- âš ï¸ ä¸åŒç‰ˆæœ¬çš„ `LlamaAttention` å†…éƒ¨å®ç°å¯èƒ½ç•¥æœ‰å·®å¼‚
- ğŸ’¡ å¦‚é‡åˆ°é—®é¢˜ï¼Œæ£€æŸ¥ hook æ•è·çš„å¼ é‡å½¢çŠ¶

### éªŒè¯å½¢çŠ¶

åœ¨ `hooks.py` ä¸­æ·»åŠ è°ƒè¯•ä»£ç ï¼š

```python
logger.debug(f"head_outputs shape: {head_outputs.shape}")  # åº”ä¸º [bs, seq_len, num_heads, head_dim]
```

### å·²çŸ¥é—®é¢˜

- Flash Attention 2 å¯èƒ½æ”¹å˜å†…éƒ¨è®¡ç®—æµç¨‹ï¼Œå¯¼è‡´ hook å¤±æ•ˆ
- è§£å†³æ–¹æ³•ï¼šè®¾ç½® `attn_implementation: null` ä½¿ç”¨æ ‡å‡†å®ç°

## å¸¸è§é—®é¢˜

### Q1: å†…å­˜ä¸è¶³

**è§£å†³æ–¹æ³•**ï¼š
- å‡å° `batch_size`
- å‡å° `max_length`
- ä½¿ç”¨ `dtype: "fp16"` æˆ– `"bf16"`
- å‡å° `max_samples`

### Q2: æ•°æ®é›†åŠ è½½å¤±è´¥

**æ£€æŸ¥**ï¼š
- æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
- JSONL æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆæ¯è¡Œä¸€ä¸ªæœ‰æ•ˆ JSONï¼‰
- HuggingFace datasets æ˜¯å¦å·²ä¸‹è½½

### Q3: Hook æ— æ³•æ•è·æ¿€æ´»

**æ£€æŸ¥**ï¼š
- æ¨¡å‹æ¶æ„æ˜¯å¦ä¸º LLaMA ç³»åˆ—
- Transformers ç‰ˆæœ¬æ˜¯å¦åœ¨ 4.50-4.59
- å°è¯•è®¾ç½® `attn_implementation: null`

### Q4: è¾“å‡ºå…¨é›¶æˆ–å¼‚å¸¸å€¼

**å¯èƒ½åŸå› **ï¼š
- Attention mask è®¾ç½®é”™è¯¯
- Token èšåˆä½ç½®è®¡ç®—é”™è¯¯
- æ¨¡å‹æƒé‡æœªæ­£ç¡®åŠ è½½

**è°ƒè¯•**ï¼šæ·»åŠ  logger.debug æ‰“å°ä¸­é—´ç»“æœ

## æœªæ¥è®­ç»ƒï¼ˆSFT/LoRAï¼‰

è™½ç„¶æœ¬é¡¹ç›®ä»…åšæ¨ç†é‡‡é›†ï¼Œä½†è®¾è®¡è€ƒè™‘äº†åç»­è®­ç»ƒçš„ä¾¿åˆ©æ€§ï¼š

### æ•°æ®é›†è¾“å‡º

æ¯ä¸ªæ ·æœ¬åŒ…å« `target_text` å­—æ®µï¼ˆä¸ `answer_letter` ç›¸åŒï¼‰ï¼Œå¯ç›´æ¥ç”¨äºè®­ç»ƒï¼š

```python
# è®­ç»ƒæ—¶çš„ label å¯¹é½
def build_sft_example(prompt_text, target_text, tokenizer):
    full_text = prompt_text + target_text
    input_ids = tokenizer(full_text)["input_ids"]
    
    # åªè®­ç»ƒ target_text éƒ¨åˆ†
    prompt_ids = tokenizer(prompt_text)["input_ids"]
    labels = [-100] * len(prompt_ids) + input_ids[len(prompt_ids):]
    
    return {"input_ids": input_ids, "labels": labels}
```

## å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œè¯·å¼•ç”¨ï¼š

```bibtex
@software{llama_head_activation_collection,
  title = {LLaMA Attention Head Activation Collection},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/your-repo}
}
```

## è®¸å¯è¯

MIT License

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤ Issue æˆ– Pull Requestã€‚

---

**æœ€åæ›´æ–°**: 2025-01-06

