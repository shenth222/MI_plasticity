# Attention Mask å¤„ç†ä¿®å¤æ€»ç»“

## é—®é¢˜åˆ†æ

ä»£ç å®¡æŸ¥åé¦ˆ**å®Œå…¨æ­£ç¡®**ã€‚åŸå®ç°ä¸ç”¨æˆ·éœ€æ±‚å­˜åœ¨ä»¥ä¸‹ä¸¥é‡ä¸ç¬¦ï¼š

### ğŸ”´ ä¸¥é‡é—®é¢˜

#### 1. Last Token èšåˆå¿½ç•¥ Attention Mask

**é—®é¢˜æè¿°**ï¼š
- åŸä»£ç  `_get_last_token_positions` å§‹ç»ˆè¿”å› `seq_len - 1`
- å®Œå…¨å¿½ç•¥ `attention_mask`ï¼Œå³ä½¿ `set_attention_mask` è¢«è°ƒç”¨
- åœ¨ ARC ä»»åŠ¡ä¸­ï¼Œä¸åŒæ ·æœ¬çš„æœ‰æ•ˆé•¿åº¦ä¸åŒï¼Œå¯¼è‡´ç³»ç»Ÿæ€§åå·®ï¼šæŠŠ padding token å½“ä½œç­”æ¡ˆ token

**ç”¨æˆ·éœ€æ±‚**ï¼š
- å¯¹æ¯ä¸ªæ ·æœ¬å–**æœ€åä¸€ä¸ªé padding çš„ token**

#### 2. All Token èšåˆåŒ…å« Padding

**é—®é¢˜æè¿°**ï¼š
- åŸä»£ç ç›´æ¥å¯¹æ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬ paddingï¼‰è¿›è¡Œå¹³å‡
- Padding token ç¨€é‡ŠçœŸå®ä¿¡å·ï¼Œå¯¼è‡´ head norm è¢«ä½ä¼°

**ç”¨æˆ·éœ€æ±‚**ï¼š
- å¯¹æ¯ä¸ªæ ·æœ¬å–**æ‰€æœ‰é padding token çš„å¹³å‡**

### ğŸŸ¡ ä¸­ç­‰é—®é¢˜

#### 3. ç¼ºå¤±å±‚é»˜è®¤å¡«å…… 0

**é—®é¢˜æè¿°**ï¼š
- å¦‚æœæŸå±‚ hook æœªè§¦å‘ï¼Œ`finalize_batch` ä¼šå¡«å…… 0
- Heatmap æ˜¾ç¤ºè¯¥å±‚ norm æå°ï¼Œè€Œä¸æ˜¯"æ— æ•°æ®"
- é™ä½å¯è§‚æµ‹æ€§ï¼Œéš¾ä»¥æ’æŸ¥

#### 4. ç»Ÿè®¡é‡ä½¿ç”¨æ‰¹å‡å€¼åŠ æƒ

**é—®é¢˜æè¿°**ï¼š
- åŸä»£ç å…ˆå¯¹ batch æ±‚å¹³å‡ï¼Œå†å–‚ç»™ Welford
- ç»“æœ = å„ batch å‡å€¼çš„ç®—æœ¯å¹³å‡
- ä¸çœŸå®"æ ·æœ¬çº§åŠ æƒå¹³å‡"ä¸åŒ
- ä¾‹å¦‚ï¼š3 ä¸ª 32-æ ·æœ¬ batch + 1 ä¸ª 2-æ ·æœ¬ batchï¼Œæƒé‡åº”è¯¥æ˜¯ 96:2ï¼Œè€Œé 3:1

## è§£å†³æ–¹æ¡ˆ

### âœ… ä¿®å¤ 1ï¼šLast Token ä½¿ç”¨ Attention Mask

**ä½ç½®**ï¼š`hooks.py` ç¬¬ 172-198 è¡Œ

**ä¿®æ”¹**ï¼š
```python
def _get_last_token_positions(self, bs: int, seq_len: int, device: torch.device = None) -> torch.Tensor:
    if self.current_attention_mask is not None:
        # ä½¿ç”¨ attention_mask æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬æœ€åä¸€ä¸ªé padding token
        mask = self.current_attention_mask.to(device)
        # å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ª 1 çš„ä½ç½®
        last_positions = mask.sum(dim=1) - 1  # [bs]
        # ç¡®ä¿è‡³å°‘ä¸º 0ï¼ˆå¤„ç†å…¨ 0 mask çš„è¾¹ç•Œæƒ…å†µï¼‰
        last_positions = torch.clamp(last_positions, min=0)
        return last_positions.long()
    else:
        # å¦‚æœæ²¡æœ‰ maskï¼Œå›é€€åˆ°ä½¿ç”¨æœ€åä¸€ä¸ªä½ç½®
        logger.warning("attention_mask æœªè®¾ç½®ï¼Œä½¿ç”¨ seq_len-1 ä½œä¸ºæœ€å token ä½ç½®")
        return torch.full((bs,), seq_len - 1, dtype=torch.long, device=device)
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ `mask.sum(dim=1) - 1` æ‰¾åˆ°æœ€åä¸€ä¸ªæœ‰æ•ˆ token ä½ç½®
- æ·»åŠ è¾¹ç•Œæ£€æŸ¥å’Œè­¦å‘Š

### âœ… ä¿®å¤ 2ï¼šAll Token è¿‡æ»¤ Padding

**ä½ç½®**ï¼š`hooks.py` ç¬¬ 200-238 è¡Œï¼ˆ`_compute_head_output_norm`ï¼‰å’Œç¬¬ 258-292 è¡Œï¼ˆ`_compute_head_resid_contrib_norm`ï¼‰

**ä¿®æ”¹**ï¼š
```python
# "all" èšåˆ - åªå¯¹æœ‰æ•ˆ token æ±‚å¹³å‡
if self.current_attention_mask is not None:
    # ä½¿ç”¨ mask è¿‡æ»¤ padding token
    mask = self.current_attention_mask.to(head_outputs.device)  # [bs, seq_len]
    mask = mask.unsqueeze(2)  # [bs, seq_len, 1]
    
    # è®¡ç®—åŠ æƒå¹³å‡ï¼šsum(norms * mask) / sum(mask)
    masked_norms = norms_per_token * mask  # [bs, seq_len, num_heads]
    sum_norms = masked_norms.sum(dim=1)  # [bs, num_heads]
    count = mask.sum(dim=1)  # [bs, 1]
    
    # é¿å…é™¤ä»¥ 0
    count = torch.clamp(count, min=1)
    norms = sum_norms / count  # [bs, num_heads]
else:
    # å¦‚æœæ²¡æœ‰ maskï¼Œå›é€€åˆ°æ‰€æœ‰ token çš„å¹³å‡
    logger.warning("attention_mask æœªè®¾ç½®ï¼Œä½¿ç”¨æ‰€æœ‰ token è¿›è¡Œèšåˆ")
    norms = norms_per_token.mean(dim=1)
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨ mask åŠ æƒå¹³å‡ï¼š`sum(values * mask) / sum(mask)`
- åŒæ—¶åº”ç”¨äº head output norm å’Œ head residual contribution norm

### âœ… ä¿®å¤ 3ï¼šç¼ºå¤±å±‚æ£€æµ‹å’ŒæŠ¥å‘Š

**ä½ç½®**ï¼š`hooks.py` ç¬¬ 303-318 è¡Œ

**ä¿®æ”¹**ï¼š
```python
# æ£€æŸ¥ç¼ºå¤±çš„å±‚
missing_layers = []
for layer_idx in range(self.num_layers):
    if layer_idx not in self._batch_head_output_norms:
        missing_layers.append(layer_idx)

if missing_layers:
    logger.warning(f"ä»¥ä¸‹å±‚æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®: {missing_layers}")
```

**å…³é”®ç‚¹**ï¼š
- è®°å½•ç¼ºå¤±å±‚å¹¶æ‰“å°è­¦å‘Š
- æé«˜å¯è§‚æµ‹æ€§ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜

### âœ… ä¿®å¤ 4ï¼šæ ·æœ¬çº§åŠ æƒç»Ÿè®¡

**ä½ç½®**ï¼š`hooks.py` ç¬¬ 160-170 è¡Œï¼ˆ`_compute_and_update_metrics`ï¼‰å’Œç¬¬ 303-340 è¡Œï¼ˆ`finalize_batch`ï¼‰

**ä¿®æ”¹**ï¼š
```python
# åœ¨ _compute_and_update_metrics ä¸­ï¼š
# å­˜å‚¨åˆ° batch ç¼“å­˜ï¼ˆä¿å­˜æ¯ä¸ªæ ·æœ¬çš„å€¼ï¼Œä¸æ±‚å¹³å‡ï¼‰
self._batch_head_output_norms[layer_idx] = head_output_norms.cpu().numpy()  # [bs, num_heads]
self._batch_head_resid_norms[layer_idx] = head_resid_contrib_norms.cpu().numpy()  # [bs, num_heads]

# åœ¨ finalize_batch ä¸­ï¼š
# å¯¹æ¯ä¸ªæ ·æœ¬æ›´æ–°ç»Ÿè®¡ï¼ˆä½¿ç”¨æ ·æœ¬çº§åŠ æƒï¼‰
for sample_idx in range(batch_size):
    # ä¸ºå½“å‰æ ·æœ¬èšåˆæ‰€æœ‰å±‚çš„æŒ‡æ ‡
    sample_head_output_norms = np.zeros((self.num_layers, self.num_heads))
    sample_head_resid_norms = np.zeros((self.num_layers, self.num_heads))
    
    for layer_idx in range(self.num_layers):
        if layer_idx in self._batch_head_output_norms:
            sample_head_output_norms[layer_idx, :] = self._batch_head_output_norms[layer_idx][sample_idx, :]
            sample_head_resid_norms[layer_idx, :] = self._batch_head_resid_norms[layer_idx][sample_idx, :]
    
    # æ›´æ–°ç»Ÿè®¡
    self.head_output_norm_stats.update(sample_head_output_norms)
    self.head_resid_contrib_norm_stats.update(sample_head_resid_norms)
```

**å…³é”®ç‚¹**ï¼š
- ä¸å†å¯¹ batch ç»´åº¦æ±‚å¹³å‡åæ›´æ–°ç»Ÿè®¡
- é€æ ·æœ¬æ›´æ–° `OnlineStats`ï¼Œç¡®ä¿æƒé‡ä¸æ ·æœ¬æ•°åŒ¹é…
- å° batch å’Œå¤§ batch æŒ‰å®é™…æ ·æœ¬æ•°åŠ æƒ

## æµ‹è¯•éªŒè¯

åˆ›å»ºäº† `test_mask_handling.py` åŒ…å« 3 ä¸ªæµ‹è¯•ï¼š

### æµ‹è¯• 1ï¼šLast Token Position è®¡ç®—

**åœºæ™¯**ï¼š
- æ ·æœ¬ 0: 5 ä¸ªæœ‰æ•ˆ tokenï¼Œæœ€åä½ç½®åº”ä¸º 4
- æ ·æœ¬ 1: 6 ä¸ªæœ‰æ•ˆ tokenï¼Œæœ€åä½ç½®åº”ä¸º 5
- æ ·æœ¬ 2: 3 ä¸ªæœ‰æ•ˆ tokenï¼Œæœ€åä½ç½®åº”ä¸º 2

**ç»“æœ**ï¼šâœ… **é€šè¿‡** - æ­£ç¡®è®¡ç®—å‡º `[4, 5, 2]`

### æµ‹è¯• 2ï¼šAll Token Padding è¿‡æ»¤

**åœºæ™¯**ï¼š
- æ ·æœ¬ 0: å‰ 3 ä¸ª token æœ‰æ•ˆ
- æ ·æœ¬ 1: å‰ 2 ä¸ª token æœ‰æ•ˆ
- éªŒè¯ padding token ä¸å‚ä¸å¹³å‡

**ç»“æœ**ï¼šâœ… **é€šè¿‡** - æ­£ç¡®è¿‡æ»¤ padding

### æµ‹è¯• 3ï¼šæ ·æœ¬çº§åŠ æƒ

**åœºæ™¯**ï¼š
- Batch 1: 3 ä¸ªæ ·æœ¬
- Batch 2: 1 ä¸ªæ ·æœ¬
- éªŒè¯ count = 4ï¼ˆæ ·æœ¬çº§ï¼‰è€Œé 2ï¼ˆæ‰¹çº§ï¼‰

**ç»“æœ**ï¼šâœ… **é€šè¿‡** - Count = 4ï¼Œç¡®è®¤ä½¿ç”¨æ ·æœ¬çº§åŠ æƒ

## å½±å“å’Œå»ºè®®

### ğŸ“Š å¯¹å†å²ç»“æœçš„å½±å“

**ä¸¥é‡å½±å“**ï¼š
- ä¹‹å‰çš„ç»“æœå¯èƒ½å­˜åœ¨ç³»ç»Ÿæ€§åå·®
- ç‰¹åˆ«æ˜¯ ARC ä»»åŠ¡ä¸­ prompt é•¿åº¦å¯å˜çš„æƒ…å†µ
- **å»ºè®®é‡æ–°è¿è¡Œæ‰€æœ‰å®éªŒ**

### ğŸ“ åç»­å·¥ä½œ

1. **è¡¥å……æµ‹è¯•è¦†ç›–**ï¼ˆä½ä¼˜å…ˆçº§ï¼‰ï¼š
   - è¾¹ç•Œæƒ…å†µï¼šå…¨ 0 maskã€å…¨ 1 mask
   - ä¸æ‰‹å·¥è®¡ç®—çš„åŸºå‡†å€¼å¯¹æ¯”
   - ä¸åŒæ¨¡å‹æ¶æ„çš„å…¼å®¹æ€§

2. **æ–‡æ¡£æ›´æ–°**ï¼š
   - æ›´æ–° README å’Œ QUICKSTART è¯´æ˜ attention_mask çš„é‡è¦æ€§
   - æ˜ç¡®è¯´æ˜ token_agg çš„è¡Œä¸º

3. **å¯é€‰å¢å¼º**ï¼š
   - æ”¯æŒæ˜¾å¼çš„ answer span æ ‡æ³¨ï¼ˆå¦‚æœéœ€è¦æ›´ç²¾ç¡®çš„æ§åˆ¶ï¼‰
   - æ·»åŠ  mask éªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬çš„ mask éƒ½åˆç†

## æ€»ç»“

âœ… **æ‰€æœ‰åé¦ˆé—®é¢˜å·²ä¿®å¤**ï¼š
1. âœ… Last token æ­£ç¡®ä½¿ç”¨ attention_mask
2. âœ… All token æ­£ç¡®è¿‡æ»¤ padding
3. âœ… ç¼ºå¤±å±‚ä¼šè¢«æ£€æµ‹å’ŒæŠ¥å‘Š
4. âœ… ç»Ÿè®¡é‡ä½¿ç”¨æ ·æœ¬çº§åŠ æƒ
5. âœ… æµ‹è¯•éªŒè¯æ‰€æœ‰ä¿®å¤æ­£ç¡®

**ä¿®å¤å‰åå¯¹æ¯”**ï¼š

| æ–¹é¢ | ä¿®å¤å‰ | ä¿®å¤å |
|------|--------|--------|
| Last token | æ€»æ˜¯å– `seq_len-1`ï¼ŒåŒ…å« padding | å–æœ€åä¸€ä¸ªæœ‰æ•ˆ token |
| All token | å¯¹æ‰€æœ‰ token å¹³å‡ï¼ŒåŒ…å« padding | åªå¯¹æœ‰æ•ˆ token å¹³å‡ |
| ç¼ºå¤±å±‚ | å¡«å…… 0ï¼Œéš¾ä»¥å‘ç° | è­¦å‘Šæ—¥å¿—ï¼Œå¯è§‚æµ‹ |
| ç»Ÿè®¡æƒé‡ | æ‰¹å‡å€¼çš„å‡å€¼ï¼ˆæƒé‡åå·®ï¼‰ | æ ·æœ¬çº§åŠ æƒï¼ˆæ­£ç¡®ï¼‰ |
| Mask ä½¿ç”¨ | æœªä½¿ç”¨ï¼ˆè™½ç„¶è®¾ç½®äº†ï¼‰ | æ­£ç¡®ä½¿ç”¨ |

**ç°åœ¨çš„å®ç°å®Œå…¨ç¬¦åˆç”¨æˆ·éœ€æ±‚**ï¼

