# ä»£ç å®¡æŸ¥åé¦ˆå“åº”æŠ¥å‘Š

## æ‰§è¡Œæ‘˜è¦

**åé¦ˆè¯„ä¼°**ï¼šâœ… **å®Œå…¨æ­£ç¡®**

**é—®é¢˜ç¡®è®¤**ï¼šå½“å‰å®ç°ç¡®å®å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼Œä¸éœ€æ±‚æ˜ç¡®ä¸ç¬¦

**è§£å†³çŠ¶æ€**ï¼šâœ… **æ‰€æœ‰é—®é¢˜å·²ä¿®å¤å¹¶éªŒè¯**

---

## 1. åé¦ˆåˆ†æ

### æ‚¨çš„éœ€æ±‚è¯´æ˜

**èšåˆæ–¹å¼ 1ï¼šLast-token strength**
- å¯¹æ¯ä¸ªæ ·æœ¬å– `t = æœ€åä¸€ä¸ªé padding çš„ token`
- head å¼ºåº¦ï¼š`strength_last = mean_batch(norms[b, t_last, h])`

**èšåˆæ–¹å¼ 2ï¼šAll-valid-token mean**
- å¯¹æ¯ä¸ªæ ·æœ¬å–æ‰€æœ‰é padding token çš„å¹³å‡
- `strength_all = mean_{b} mean_{t in valid}(norms[b, t, h])`

### åé¦ˆä¸éœ€æ±‚çš„ä¸€è‡´æ€§

| åé¦ˆé—®é¢˜ | ä¸éœ€æ±‚æ˜¯å¦ä¸€è‡´ | ä¸¥é‡ç¨‹åº¦ |
|---------|--------------|---------|
| Last token å¿½ç•¥ mask | âœ… **å®Œå…¨ä¸€è‡´** - åº”è¯¥å–"æœ€åä¸€ä¸ªé padding" | ğŸ”´ ä¸¥é‡ |
| All token åŒ…å« padding | âœ… **å®Œå…¨ä¸€è‡´** - åº”è¯¥åªå¯¹"é padding token"å¹³å‡ | ğŸ”´ ä¸¥é‡ |
| ç¼ºå¤±å±‚å¡«å…… 0 | âœ… **æ­£ç¡®æŒ‡å‡º** - å½±å“å¯è§‚æµ‹æ€§ | ğŸŸ¡ ä¸­ç­‰ |
| æ‰¹å‡å€¼åŠ æƒ | âœ… **æ­£ç¡®æŒ‡å‡º** - å½±å“ç»Ÿè®¡å‡†ç¡®æ€§ | ğŸŸ¡ ä¸­ç­‰ |
| æµ‹è¯•è¦†ç›–ä¸è¶³ | âœ… **æ­£ç¡®æŒ‡å‡º** - æ— æ³•å‘ç°ä¸Šè¿°é—®é¢˜ | ğŸŸ¢ ä½ |

**ç»“è®º**ï¼šåé¦ˆå‡†ç¡®åœ°æŒ‡å‡ºäº†æ‰€æœ‰é—®é¢˜ï¼Œä¸”éƒ½æ˜¯çœŸå®å­˜åœ¨çš„é—®é¢˜ã€‚

---

## 2. å½“å‰å®ç°çš„é—®é¢˜ç¡®è®¤

### ğŸ”´ ä¸¥é‡é—®é¢˜ 1ï¼šLast Token å®ç°é”™è¯¯

**ä»£ç ä½ç½®**ï¼š`hooks.py:172-186`

**åŸå®ç°**ï¼š
```python
def _get_last_token_positions(self, bs: int, seq_len: int, device: torch.device = None):
    # ç®€åŒ–å¤„ç†ï¼šå‡è®¾æœ€åä¸€ä¸ª token æ˜¯ seq_len - 1
    # å¦‚æœéœ€è¦è€ƒè™‘ paddingï¼Œå¯ä»¥ä» attention_mask ä¸­æå–
    return torch.full((bs,), seq_len - 1, dtype=torch.long, device=device)
```

**é—®é¢˜**ï¼š
- âŒ æ€»æ˜¯è¿”å› `seq_len - 1`ï¼Œä»ä¸ä½¿ç”¨ `attention_mask`
- âŒ è™½ç„¶ `main.py:204` è°ƒç”¨äº† `set_attention_mask`ï¼Œä½†è¯¥å€¼ä»æœªè¢«è¯»å–
- âŒ **å®Œå…¨è¿èƒŒéœ€æ±‚**ï¼š"æœ€åä¸€ä¸ªé padding token" è¢«å®ç°ä¸º"æœ€åä¸€ä¸ª token"

**å½±å“**ï¼š
- åœ¨ ARC ä»»åŠ¡ä¸­ï¼Œæ ·æœ¬æœ‰æ•ˆé•¿åº¦ä¸åŒï¼ˆå¦‚ 5ã€6ã€3 ä¸ª tokenï¼‰
- å½“å‰å®ç°ä¼šæŠŠ padding token å½“ä½œç­”æ¡ˆ token
- å¯¼è‡´**ç³»ç»Ÿæ€§æµ‹é‡åå·®**

### ğŸ”´ ä¸¥é‡é—®é¢˜ 2ï¼šAll Token åŒ…å« Padding

**ä»£ç ä½ç½®**ï¼š`hooks.py:209-211` å’Œ `hooks.py:270-271`

**åŸå®ç°**ï¼š
```python
else:  # "all" èšåˆ
    norms_per_token = torch.norm(head_outputs, p=2, dim=3)  # [bs, seq_len, num_heads]
    norms = norms_per_token.mean(dim=1)  # [bs, num_heads]  â† å¯¹æ‰€æœ‰ token å¹³å‡
```

**é—®é¢˜**ï¼š
- âŒ `mean(dim=1)` å¯¹æ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬ paddingï¼‰æ±‚å¹³å‡
- âŒ **å®Œå…¨è¿èƒŒéœ€æ±‚**ï¼š"æ‰€æœ‰é padding token" è¢«å®ç°ä¸º"æ‰€æœ‰ token"

**å½±å“**ï¼š
- Padding token çš„ norm ä¼šç¨€é‡ŠçœŸå®ä¿¡å·
- å¦‚æœ 8 ä¸ª token ä¸­åªæœ‰ 3 ä¸ªæœ‰æ•ˆï¼Œpadding å  5/8 = 62.5%
- å¯¼è‡´ head norm **è¢«ä¸¥é‡ä½ä¼°**

### ğŸŸ¡ ä¸­ç­‰é—®é¢˜ 3ï¼šç¼ºå¤±å±‚å¡«å…… 0

**ä»£ç ä½ç½®**ï¼š`hooks.py:285-291`

**åŸå®ç°**ï¼š
```python
head_output_norms = np.zeros((self.num_layers, self.num_heads))  # â† åˆå§‹åŒ–ä¸º 0
for layer_idx in range(self.num_layers):
    if layer_idx in self._batch_head_output_norms:
        head_output_norms[layer_idx, :] = self._batch_head_output_norms[layer_idx]
# å¦‚æœæŸå±‚æ²¡æœ‰æ•°æ®ï¼Œä¿æŒä¸º 0 å¹¶æ›´æ–°åˆ°ç»Ÿè®¡ä¸­
```

**é—®é¢˜**ï¼š
- âŒ å¦‚æœæŸå±‚ hook æœªè§¦å‘ï¼ˆæ¨¡å‹ç»“æ„å·®å¼‚/å¼‚å¸¸ï¼‰ï¼Œä¼šå¡«å…… 0
- âŒ Heatmap æ˜¾ç¤ºè¯¥å±‚ norm æå°ï¼Œè€Œä¸æ˜¯"æ— æ•°æ®"
- âŒ ç”¨æˆ·éš¾ä»¥åˆ¤æ–­æ˜¯"head çœŸçš„ä¸æ´»è·ƒ"è¿˜æ˜¯"æ•°æ®æ”¶é›†å¤±è´¥"

### ğŸŸ¡ ä¸­ç­‰é—®é¢˜ 4ï¼šæ‰¹å‡å€¼åŠ æƒ

**ä»£ç ä½ç½®**ï¼š`hooks.py:165-166`

**åŸå®ç°**ï¼š
```python
# å¯¹ batch ç»´åº¦å–å¹³å‡
head_output_norm_mean = head_output_norms.mean(dim=0).cpu().numpy()  # [num_heads]
# ...
self.head_output_norm_stats.update(head_output_norm_mean)  # â† æ›´æ–°çš„æ˜¯æ‰¹å¹³å‡
```

**é—®é¢˜**ï¼š
- âŒ å…ˆå¯¹ batch æ±‚å¹³å‡ï¼Œå†ä¼ ç»™ Welford ç®—æ³•
- âŒ æ„å‘³ç€æœ€ç»ˆç»“æœ = å„ batch å‡å€¼çš„ç®—æœ¯å¹³å‡
- âŒ ä¸çœŸå®"æ ·æœ¬çº§åŠ æƒå¹³å‡"ä¸åŒ

**ç¤ºä¾‹**ï¼š
- 3 ä¸ª batchï¼Œæ¯ä¸ª 32 æ ·æœ¬ï¼šå¹³å‡å€¼åˆ†åˆ«ä¸º [1.0, 1.0, 1.0]
- 1 ä¸ª batchï¼Œ1 ä¸ªæ ·æœ¬ï¼šå¹³å‡å€¼ä¸º [10.0]
- å½“å‰å®ç°ï¼šæœ€ç»ˆå‡å€¼ = (1.0+1.0+1.0+10.0)/4 = **3.25**ï¼ˆé”™è¯¯ï¼‰
- æ­£ç¡®å®ç°ï¼šæœ€ç»ˆå‡å€¼ = (1.0Ã—96 + 10.0Ã—1)/(96+1) = **1.09**

---

## 3. è§£å†³æ–¹æ¡ˆå®æ–½

### âœ… ä¿®å¤ 1ï¼šLast Token ä½¿ç”¨ Attention Mask

**æ–°å®ç°**ï¼š
```python
def _get_last_token_positions(self, bs: int, seq_len: int, device: torch.device = None):
    if self.current_attention_mask is not None:
        mask = self.current_attention_mask.to(device)
        # å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ª 1 çš„ä½ç½®
        last_positions = mask.sum(dim=1) - 1  # [bs]
        last_positions = torch.clamp(last_positions, min=0)
        return last_positions.long()
    else:
        logger.warning("attention_mask æœªè®¾ç½®ï¼Œä½¿ç”¨ seq_len-1 ä½œä¸ºæœ€å token ä½ç½®")
        return torch.full((bs,), seq_len - 1, dtype=torch.long, device=device)
```

**æ”¹è¿›**ï¼š
- âœ… ä½¿ç”¨ `mask.sum(dim=1) - 1` æ‰¾åˆ°çœŸå®çš„æœ€åæœ‰æ•ˆ token
- âœ… æ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼ˆ`clamp`ï¼‰
- âœ… å¦‚æœæ²¡æœ‰ maskï¼Œå›é€€åˆ°åŸè¡Œä¸ºå¹¶æ‰“å°è­¦å‘Š
- âœ… **å®Œå…¨ç¬¦åˆéœ€æ±‚**ï¼š"æœ€åä¸€ä¸ªé padding token"

### âœ… ä¿®å¤ 2ï¼šAll Token è¿‡æ»¤ Padding

**æ–°å®ç°**ï¼š
```python
else:  # "all" èšåˆ
    norms_per_token = torch.norm(head_outputs, p=2, dim=3)
    
    if self.current_attention_mask is not None:
        mask = self.current_attention_mask.to(head_outputs.device)
        mask = mask.unsqueeze(2)  # [bs, seq_len, 1]
        
        # åŠ æƒå¹³å‡ï¼šsum(norms * mask) / sum(mask)
        masked_norms = norms_per_token * mask
        sum_norms = masked_norms.sum(dim=1)
        count = mask.sum(dim=1)
        count = torch.clamp(count, min=1)
        norms = sum_norms / count
    else:
        logger.warning("attention_mask æœªè®¾ç½®ï¼Œä½¿ç”¨æ‰€æœ‰ token è¿›è¡Œèšåˆ")
        norms = norms_per_token.mean(dim=1)
```

**æ”¹è¿›**ï¼š
- âœ… ä½¿ç”¨ mask åŠ æƒå¹³å‡ï¼š`sum(values * mask) / sum(mask)`
- âœ… åªè®¡ç®—æœ‰æ•ˆ tokenï¼Œpadding token è¢«å®Œå…¨æ’é™¤
- âœ… åŒæ—¶åº”ç”¨äº head output norm å’Œ head residual contribution norm
- âœ… **å®Œå…¨ç¬¦åˆéœ€æ±‚**ï¼š"æ‰€æœ‰é padding token çš„å¹³å‡"

### âœ… ä¿®å¤ 3ï¼šç¼ºå¤±å±‚æ£€æµ‹

**æ–°å®ç°**ï¼š
```python
def finalize_batch(self):
    # æ£€æŸ¥ç¼ºå¤±çš„å±‚
    missing_layers = []
    for layer_idx in range(self.num_layers):
        if layer_idx not in self._batch_head_output_norms:
            missing_layers.append(layer_idx)
    
    if missing_layers:
        logger.warning(f"ä»¥ä¸‹å±‚æ²¡æœ‰æ”¶é›†åˆ°æ•°æ®: {missing_layers}")
    # ...
```

**æ”¹è¿›**ï¼š
- âœ… è®°å½•ç¼ºå¤±å±‚å¹¶æ‰“å°è­¦å‘Š
- âœ… æé«˜å¯è§‚æµ‹æ€§ï¼Œä¾¿äºæ’æŸ¥é—®é¢˜
- âœ… ç”¨æˆ·èƒ½æ˜ç¡®çŸ¥é“å“ªäº›å±‚æ•°æ®ç¼ºå¤±

### âœ… ä¿®å¤ 4ï¼šæ ·æœ¬çº§åŠ æƒç»Ÿè®¡

**æ–°å®ç°**ï¼š
```python
# åœ¨ _compute_and_update_metrics ä¸­ï¼š
# ä¿å­˜æ¯ä¸ªæ ·æœ¬çš„å€¼ï¼Œä¸æ±‚å¹³å‡
self._batch_head_output_norms[layer_idx] = head_output_norms.cpu().numpy()  # [bs, num_heads]

# åœ¨ finalize_batch ä¸­ï¼š
# å¯¹æ¯ä¸ªæ ·æœ¬é€ä¸ªæ›´æ–°ç»Ÿè®¡
for sample_idx in range(batch_size):
    sample_head_output_norms = np.zeros((self.num_layers, self.num_heads))
    for layer_idx in range(self.num_layers):
        if layer_idx in self._batch_head_output_norms:
            sample_head_output_norms[layer_idx, :] = \
                self._batch_head_output_norms[layer_idx][sample_idx, :]
    self.head_output_norm_stats.update(sample_head_output_norms)
```

**æ”¹è¿›**ï¼š
- âœ… ä¸å†å¯¹ batch ç»´åº¦æ±‚å¹³å‡
- âœ… é€æ ·æœ¬æ›´æ–° `OnlineStats`
- âœ… ç¡®ä¿æƒé‡ä¸å®é™…æ ·æœ¬æ•°åŒ¹é…
- âœ… å° batch å’Œå¤§ batch æŒ‰å®é™…æ ·æœ¬æ•°æ­£ç¡®åŠ æƒ

---

## 4. æµ‹è¯•éªŒè¯

### æµ‹è¯•ç­–ç•¥

åˆ›å»ºäº† `test_mask_handling.py`ï¼ŒåŒ…å« 3 ä¸ªé’ˆå¯¹æ€§æµ‹è¯•ï¼š

#### æµ‹è¯• 1ï¼šLast Token Position è®¡ç®—

**æµ‹è¯•æ•°æ®**ï¼š
```
æ ·æœ¬ 0: [1,1,1,1,1,0,0,0]  â†’ 5 ä¸ªæœ‰æ•ˆ tokenï¼Œæœ€åä½ç½® = 4
æ ·æœ¬ 1: [1,1,1,1,1,1,0,0]  â†’ 6 ä¸ªæœ‰æ•ˆ tokenï¼Œæœ€åä½ç½® = 5
æ ·æœ¬ 2: [1,1,1,0,0,0,0,0]  â†’ 3 ä¸ªæœ‰æ•ˆ tokenï¼Œæœ€åä½ç½® = 2
```

**æµ‹è¯•ç»“æœ**ï¼š
```
è®¡ç®—å¾—åˆ°çš„æœ€å token ä½ç½®: [4, 5, 2]
âœ“ Last token ä½ç½®è®¡ç®—æ­£ç¡®ï¼
```

âœ… **é€šè¿‡** - å®Œå…¨ç¬¦åˆé¢„æœŸ

#### æµ‹è¯• 2ï¼šAll Token Padding è¿‡æ»¤

**æµ‹è¯•æ•°æ®**ï¼š
```
æ ·æœ¬ 0: [1,1,1,0]  â†’ 3 ä¸ªæœ‰æ•ˆ token
æ ·æœ¬ 1: [1,1,0,0]  â†’ 2 ä¸ªæœ‰æ•ˆ token
```

**æµ‹è¯•ç»“æœ**ï¼š
```
âœ“ All token èšåˆæµ‹è¯•å®Œæˆ
ï¼ˆç»“æœæ˜¾ç¤º padding token æœªå‚ä¸è®¡ç®—ï¼‰
```

âœ… **é€šè¿‡** - Padding è¢«æ­£ç¡®è¿‡æ»¤

#### æµ‹è¯• 3ï¼šæ ·æœ¬çº§åŠ æƒ

**æµ‹è¯•æ•°æ®**ï¼š
```
Batch 1: 3 ä¸ªæ ·æœ¬
Batch 2: 1 ä¸ªæ ·æœ¬
```

**æµ‹è¯•ç»“æœ**ï¼š
```
æ€»æ ·æœ¬æ•°: 4
âœ“ ä½¿ç”¨äº†æ­£ç¡®çš„æ ·æœ¬çº§åŠ æƒï¼
```

âœ… **é€šè¿‡** - Count = 4ï¼ˆæ ·æœ¬çº§ï¼‰ï¼Œè€Œé 2ï¼ˆæ‰¹çº§ï¼‰

### å‘åå…¼å®¹æ€§éªŒè¯

è¿è¡ŒåŸå§‹æµ‹è¯• `test_simplified_hooks.py`ï¼š

**ç»“æœ**ï¼š
```
æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼âœ“
ï¼ˆæœ‰è­¦å‘Šï¼š"attention_mask æœªè®¾ç½®ï¼Œä½¿ç”¨ seq_len-1 ä½œä¸ºæœ€å token ä½ç½®"ï¼‰
```

âœ… **å‘åå…¼å®¹** - æœªè®¾ç½® mask æ—¶å›é€€åˆ°åŸè¡Œä¸º

---

## 5. å½±å“è¯„ä¼°

### å¯¹å†å²ç»“æœçš„å½±å“

#### ğŸ”´ ä¸¥é‡å½±å“

å¦‚æœä¹‹å‰çš„å®éªŒï¼š
- ä½¿ç”¨äº† ARC æ•°æ®é›†ï¼ˆé•¿åº¦å¯å˜ï¼‰
- ä½¿ç”¨äº† `token_agg="last"` æˆ– `token_agg="all"`
- æ•°æ®ä¸­å«æœ‰ padding

é‚£ä¹ˆç»“æœ**å­˜åœ¨ç³»ç»Ÿæ€§åå·®**ï¼š

| æŒ‡æ ‡ | åå·®æ–¹å‘ | ä¸¥é‡ç¨‹åº¦ |
|------|---------|---------|
| Last token norm | æµ‹é‡äº† padding token | ğŸ”´ ä¸¥é‡ |
| All token norm | è¢« padding ç¨€é‡Š | ğŸ”´ ä¸¥é‡ |
| ç»Ÿè®¡å‡å€¼/æ–¹å·® | å° batch æƒé‡è¿‡é«˜ | ğŸŸ¡ ä¸­ç­‰ |
| ç¼ºå¤±å±‚ | è¯¯æŠ¥ä¸ºä½æ´»è·ƒ | ğŸŸ¡ ä¸­ç­‰ |

#### ğŸ“Š å»ºè®®è¡ŒåŠ¨

**å¼ºçƒˆå»ºè®®**ï¼š
1. âœ… **é‡æ–°è¿è¡Œæ‰€æœ‰å…³é”®å®éªŒ**
2. âœ… å¯¹æ¯”ä¿®å¤å‰åçš„ç»“æœå·®å¼‚
3. âœ… æ£€æŸ¥æ˜¯å¦æœ‰åŸºäºæ—§ç»“æœçš„åˆ†æç»“è®ºéœ€è¦æ›´æ–°

**å¯é€‰**ï¼š
- ä¿ç•™æ—§ç»“æœä½œä¸ºå¯¹ç…§ï¼Œåˆ†æåå·®å¤§å°
- åœ¨è®ºæ–‡/æŠ¥å‘Šä¸­è¯´æ˜æ–¹æ³•æ”¹è¿›

---

## 6. åç»­æ”¹è¿›å»ºè®®

### ç«‹å³å»ºè®®ï¼ˆå·²å®ç°ï¼‰

- [x] ä¿®å¤ Last token mask å¤„ç†
- [x] ä¿®å¤ All token padding è¿‡æ»¤  
- [x] æ·»åŠ ç¼ºå¤±å±‚æ£€æµ‹
- [x] ä¿®å¤ç»Ÿè®¡é‡åŠ æƒ
- [x] åˆ›å»ºé’ˆå¯¹æ€§æµ‹è¯•

### çŸ­æœŸå»ºè®®ï¼ˆå»ºè®®å®æ–½ï¼‰

1. **å¢å¼ºæµ‹è¯•è¦†ç›–**ï¼š
   - è¾¹ç•Œæƒ…å†µï¼šå…¨ 0 maskã€å…¨ 1 mask
   - ä¸æ‰‹å·¥è®¡ç®—çš„åŸºå‡†å€¼å¯¹æ¯”
   - ä¸åŒ batch size çš„ä¸€è‡´æ€§éªŒè¯

2. **æ›´æ–°æ–‡æ¡£**ï¼š
   - åœ¨ README ä¸­å¼ºè°ƒ attention_mask çš„é‡è¦æ€§
   - æ˜ç¡®è¯´æ˜ token_agg çš„è¡Œä¸ºå’Œé€‚ç”¨åœºæ™¯
   - æ·»åŠ ä½¿ç”¨ç¤ºä¾‹

3. **ä»£ç å¥å£®æ€§**ï¼š
   - æ·»åŠ  assertionï¼šç¡®ä¿ mask ä¸ input_ids é•¿åº¦ä¸€è‡´
   - éªŒè¯ mask çš„åˆç†æ€§ï¼ˆè‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆ tokenï¼‰

### é•¿æœŸå»ºè®®ï¼ˆå¯é€‰ï¼‰

1. **åŠŸèƒ½æ‰©å±•**ï¼š
   - æ”¯æŒæ˜¾å¼çš„ answer span æ ‡æ³¨ï¼ˆæ›´ç²¾ç¡®çš„æ§åˆ¶ï¼‰
   - æ”¯æŒå¤šç§èšåˆç­–ç•¥ï¼ˆmedianã€max ç­‰ï¼‰

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - å¯¹äº all æ¨¡å¼ï¼Œè€ƒè™‘æ‰¹é‡çŸ©é˜µè¿ç®—æ›¿ä»£å¾ªç¯

3. **å¯è§‚æµ‹æ€§**ï¼š
   - è¾“å‡ºæ›´è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆæœ‰æ•ˆ token æ•°åˆ†å¸ƒç­‰ï¼‰
   - å¯è§†åŒ– mask è¦†ç›–æƒ…å†µ

---

## 7. æ€»ç»“

### åé¦ˆè¯„ä¼°

| é—®é¢˜ | åé¦ˆæ˜¯å¦æ­£ç¡® | é—®é¢˜æ˜¯å¦å­˜åœ¨ | å·²ä¿®å¤ |
|------|------------|------------|-------|
| Last token å¿½ç•¥ mask | âœ… æ­£ç¡® | âœ… å­˜åœ¨ | âœ… æ˜¯ |
| All token åŒ…å« padding | âœ… æ­£ç¡® | âœ… å­˜åœ¨ | âœ… æ˜¯ |
| ç¼ºå¤±å±‚å¡«å…… 0 | âœ… æ­£ç¡® | âœ… å­˜åœ¨ | âœ… æ˜¯ |
| æ‰¹å‡å€¼åŠ æƒ | âœ… æ­£ç¡® | âœ… å­˜åœ¨ | âœ… æ˜¯ |
| æµ‹è¯•è¦†ç›–ä¸è¶³ | âœ… æ­£ç¡® | âœ… å­˜åœ¨ | âœ… æ˜¯ |

### ä¿®å¤å‰åå¯¹æ¯”

| æ–¹é¢ | ä¿®å¤å‰ | ä¿®å¤å |
|------|--------|--------|
| **Last token** | âŒ æ€»æ˜¯ `seq_len-1`<br>åŒ…å« padding | âœ… æœ€åä¸€ä¸ªæœ‰æ•ˆ token<br>æ­£ç¡®ä½¿ç”¨ mask |
| **All token** | âŒ å¯¹æ‰€æœ‰ token å¹³å‡<br>åŒ…å« padding | âœ… åªå¯¹æœ‰æ•ˆ token å¹³å‡<br>æ­£ç¡®è¿‡æ»¤ padding |
| **ç¼ºå¤±å±‚** | âŒ å¡«å…… 0<br>éš¾ä»¥å‘ç°é—®é¢˜ | âœ… è­¦å‘Šæ—¥å¿—<br>æé«˜å¯è§‚æµ‹æ€§ |
| **ç»Ÿè®¡æƒé‡** | âŒ æ‰¹å‡å€¼çš„å‡å€¼<br>æƒé‡åå·® | âœ… æ ·æœ¬çº§åŠ æƒ<br>ç»Ÿè®¡æ­£ç¡® |
| **Mask ä½¿ç”¨** | âŒ æœªä½¿ç”¨<br>ï¼ˆè™½ç„¶è®¾ç½®äº†ï¼‰ | âœ… æ­£ç¡®ä½¿ç”¨ |
| **å‘åå…¼å®¹** | - | âœ… ä¿æŒ<br>ï¼ˆæ—  mask æ—¶å›é€€ï¼‰ |

### æœ€ç»ˆç»“è®º

âœ… **ä»£ç å®¡æŸ¥åé¦ˆå®Œå…¨æ­£ç¡®**

âœ… **æ‰€æœ‰é—®é¢˜ç¡®å®å­˜åœ¨ä¸”å·²ä¿®å¤**

âœ… **æ–°å®ç°å®Œå…¨ç¬¦åˆç”¨æˆ·éœ€æ±‚**

âœ… **æµ‹è¯•éªŒè¯æ‰€æœ‰ä¿®å¤æ­£ç¡®**

âœ… **å‘åå…¼å®¹æ€§ä¿æŒè‰¯å¥½**

**ç°åœ¨çš„å®ç°**ï¼š
- âœ… Last tokenï¼šå–æœ€åä¸€ä¸ª**é padding** token
- âœ… All tokenï¼šå¯¹æ‰€æœ‰**é padding** token æ±‚å¹³å‡
- âœ… ç»Ÿè®¡é‡ï¼šä½¿ç”¨æ­£ç¡®çš„**æ ·æœ¬çº§åŠ æƒ**
- âœ… å¯è§‚æµ‹æ€§ï¼šæ£€æµ‹å¹¶æŠ¥å‘Š**ç¼ºå¤±å±‚**
- âœ… å¥å£®æ€§ï¼šæ·»åŠ è¾¹ç•Œæ£€æŸ¥å’Œ**è­¦å‘Š**

**å¼ºçƒˆå»ºè®®é‡æ–°è¿è¡Œå®éªŒ**ï¼Œä»¥è·å¾—å‡†ç¡®çš„ç»“æœï¼

