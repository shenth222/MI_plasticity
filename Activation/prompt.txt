你是一个资深的 LLM 工程与科研复现专家。请为我生成一个可运行的 Python 项目（面向 transformers==4.5x），用于在 decoder-only 模型（LLaMA 3.2-1B）上采集每层每个 attention head 的两类强度信号：
(1) Head Output Norm
(2) Head Residual Contribution Norm

## 0. 约束与背景
- 模型：LLaMA 3.2 1B，使用本地路径加载（例如 /data/models/llama-3.2-1b/），tokenizer 也本地。
- 数据：ARC-Challenge，使用本地数据（jsonl 或者 HF datasets 的本地 cache）。我会提供一个 data_dir（例如 /data/datasets/arc_challenge/）。
- transformers：4.5x（以 4.50~4.59 的实现差异为容忍范围）
- 运行环境：单机单卡/多卡都可能；但你先实现单卡可运行（支持 device_map="auto" 和 torch_dtype=bfloat16/fp16）。
- 目标：只做推理采集，不做训练；但是 prompt 模板要同时适用于后续做全量微调与 LoRA 微调（SFT/Instruction tuning），因此模板要“训练友好”和“推理友好”并尽量提升效果。

## 1. 项目结构要求
生成如下目录结构，并给出每个文件的完整代码：

project/
  README.md
  requirements.txt
  configs/
    default.yaml
  src/
    __init__.py
    main.py
    config.py
    data/
      __init__.py
      dataset_base.py
      arc.py
      prompt.py
    model/
      __init__.py
      loader.py
      hooks.py
      metrics.py
    utils/
      __init__.py
      seed.py
      io.py
      logging.py
  scripts/
    run_arc_collect.sh
  outputs/
    (运行时自动生成)

## 2. Prompt 模板（核心要求：同时适配 全量微调 / LoRA 微调，且对 ARC 这种多选题效果要好）
### 2.1 统一格式：Instruction + Input + Output（SFT 友好）
你需要在 src/data/prompt.py 中实现一个 PromptBuilder，支持：
- "arc_mcq_v1" 模板（默认）
- 可扩展模板 registry（未来加 commonsenseqa/boolq 等）

### 2.2 arc_mcq_v1 模板必须满足：
- 清晰的任务指令（强调“选择最正确的选项字母”，减少模型输出冗余）
- 强约束输出格式：最终只输出一个大写字母（A/B/C/D），并允许在训练时用标签对齐
- 支持 few-shot = 0/1/2（默认 0），但结构预留
- 支持 chain-of-thought 的“隐藏推理”策略：prompt 中不要求模型输出推理过程，只输出字母（避免训练时被迫生成长推理导致偏移）；但可以通过可选参数让模型在内部推理（例如加一句“Think step-by-step privately, then answer with a single letter.”）
- 对全量微调与 LoRA 微调都要友好（避免过长、避免过多系统消息，兼容 chat/non-chat 模型）

给出模板示例（必须落地到代码里）：

[INSTRUCTION]
You are a careful reasoner. Read the question and choose the single best answer from the options.
Think step-by-step privately, but do not reveal your reasoning.
Return only the letter of the correct option (A, B, C, or D).

[INPUT]
Question: {question}
Options:
A. {A}
B. {B}
C. {C}
D. {D}

[OUTPUT]
Answer: 

并确保 tokenizer 在末尾不会自动加多余 tokens；labels 对齐时只训练模型生成正确的单个字母（未来训练用）。

## 3. 数据层实现（当前仅 ARC-Challenge，但要可扩展）
- 定义 DatasetBase（src/data/dataset_base.py）统一接口：
  - __len__/__getitem__
  - format_example(example) -> dict 包含 prompt_text, answer_letter, meta
- 实现 ARCDataset（src/data/arc.py）：
  - 支持从 data_dir 读取（优先 jsonl），若无则允许从 datasets.load_dataset("ai2_arc","ARC-Challenge") 但必须允许指定 cache_dir 为本地（并提示用户如何离线准备）
  - 统一把选项整理为 A/B/C/D（ARC 有时是 4 或 5 个选项；若出现 5 个选项 E，要么丢弃该样本要么动态扩展模板；默认策略：若非 4 选项则跳过，并记录日志计数）
  - 输出 answer_letter（A/B/C/D）

## 4. 模型加载与 hook（核心工程）
### 4.1 模型加载
src/model/loader.py:
- load_model_tokenizer(model_path, dtype, device_map, attn_implementation 可选)
- model.eval(), torch.set_grad_enabled(False)
- 返回 model, tokenizer

### 4.2 Hook 目标：获取每层每个 head 的两个指标
你需要实现一个 HookManager（src/model/hooks.py），能够在 forward 时为每个 layer 采集：
(1) Head Output Norm:
  - 需要拿到每层 attention 中 “合并 heads 前”的 per-head attn_output, 即 attention_weight @ V 的结果
  - 形状通常为 [bs, num_heads, q_len, head_dim] 或 [bs, q_len, num_heads, head_dim]
  - 计算每个 head 在指定 token 位置的 L2 norm，然后做 batch 聚合（在线累计均值/方差）

(2) Head Residual Contribution Norm:
  - 对每个 head 的输出，计算其经过 o_proj 后在 hidden_dim 空间的贡献向量，再取 L2 norm
  - 关键：o_proj 是一个线性层 W_o: [hidden, hidden]
    - multihead 拼接后: concat(head_outputs) 形状 [bs, q_len, hidden]
    - o_proj(concat) -> [bs, q_len, hidden]
  - 对单个 head 的贡献：只取 W_o 中对应该 head slice 的权重矩阵：
    - hidden_dim = num_heads * head_dim
    - 对 head h，对应输入 slice 为 [h*head_dim:(h+1)*head_dim]
    - 贡献 = head_output[h] @ W_o_slice^T （注意维度）
    - 这样得到 [bs, q_len, hidden_dim] 的贡献向量
    - 取 L2 norm 即 residual contribution norm
  - 注意实现要高效：不要为每个 head 都构造巨大张量；建议用 einsum 或 matmul，并且支持分块计算或只在指定 token 位置计算。

### 4.3 Token 位置聚合策略（必须同时支持两种）
在 configs/default.yaml 提供参数 token_agg:
- "last": 每个样本取最后一个有效 token（attention mask 最后一个 1 的位置）
- "all": 对所有有效 token 取平均

实现时：
- 对 "last"：对每个样本找到 last_idx，然后只在该 idx 取 norm
- 对 "all"：对所有 mask=1 的 token 取平均（避免 padding 污染）

### 4.4 在线统计（避免存大 tensor）
在 src/model/metrics.py 实现 OnlineStats（Welford）：
- update(x): x 可以是 numpy/torch 张量
- 支持按 (layer, head) 维度累计 mean/var/count
- 最终能导出：
  - head_output_norm_mean[layer, head]
  - head_resid_contrib_norm_mean[layer, head]
  - 以及可选方差与样本数

## 5. 主程序（main.py）
src/main.py 实现命令行入口：
参数：
- --model_path
- --data_dir
- --output_dir
- --max_samples (默认 5000，-1 表示全量)
- --batch_size (默认 4)
- --max_length (默认 256/384)
- --dtype (bf16/fp16/fp32)
- --device_map (auto/cuda)
- --token_agg (last/all)
- --template_name (arc_mcq_v1)
- --few_shot (0/1/2)
- --seed
- --save_every (可选)

流程：
1) load config + set seed
2) load dataset -> dataloader（collate：tokenize prompt_text；保留 attention_mask；不需要 labels）
3) load model/tokenizer
4) attach hooks (HookManager)
5) 对 dataloader 做前向：
   - with torch.no_grad()
   - model(input_ids, attention_mask, use_cache=False, output_attentions=False)
   - HookManager 在 forward 中自动采集并更新 OnlineStats
6) 结束后保存：
   - outputs/xxx/head_output_norm_mean.npy
   - outputs/xxx/head_resid_contrib_norm_mean.npy
   - outputs/xxx/meta.json（包含模型信息、dtype、token_agg、样本数等）
   - 另外生成一张热力图（matplotlib，不指定颜色风格，单图）：
     - layers x heads 的 mean 值 heatmap（分别对两种指标各生成一张 png）

## 6. 兼容性与鲁棒性要求
- transformers 4.5x 里 LlamaAttention 的内部变量命名可能变化，你需要写“适配层”：
  - 优先在 forward hook 中捕获 attention module 的输入/输出，并尽可能从输出中恢复 per-head 输出
  - 如果只能拿到 merged output，则：
    - 使用 reshape 恢复 [bs, q_len, num_heads, head_dim]（前提是未经过 o_proj；如果已经过，需在 o_proj 前 hook）
  - 代码要在 README 里说明：你 hook 的位置是哪个模块（例如 model.model.layers[i].self_attn），以及如何在不同版本中验证形状。

- 性能：
  - Residual Contribution Norm 只在 token_agg 指定的 token 位置计算，避免 O(bs*seq*heads*hidden) 爆炸
  - 支持 gradient checkpointing 不需要；use_cache=False

- 日志：
  - tqdm 进度条
  - 记录跳过样本数量（比如非 4 选项）
  - 每 N step 打印一次当前均值范围

## 7. README 要求
README.md 必须包含：
- 环境安装（requirements）
- 如何准备本地 ARC-Challenge 数据（给出 jsonl 格式示例字段，或 datasets 离线缓存说明）
- 如何运行（bash 示例）
- 输出文件说明
- 如何扩展到其他数据集（新增 data/xxx.py + prompt 模板）

## 8. 代码质量要求
- 代码必须可运行（不要伪代码）
- 关键函数要有类型注解
- 遇到缺失数据/字段要报清晰错误
- 允许用户只改 configs/default.yaml 即可跑通
- 不要使用 seaborn；画图用 matplotlib；不指定颜色参数

请按上述要求生成项目完整代码。

补充/修改要求：ARC-Challenge 选项可能为 4 或 5 个（含 E），项目必须原生支持 A–D 与 A–E，默认不跳过 5 选项样本。

## A. 数据处理修改（src/data/arc.py）
1) 不再假设只有 4 个选项。将 choices 标准化为：
   - option_labels: List[str]，例如 ["A","B","C","D"] 或 ["A","B","C","D","E"]
   - option_texts:  Dict[str,str]，key 为 label
2) ARC 原始数据中 answerKey 可能是 "A"/"B"/... 也可能是 "1"/"2"/...（如出现数字），你需要实现 robust 映射：
   - 若 answerKey 是字母，直接用；
   - 若是数字字符串 n，从 1 开始映射到 labels[n-1]；
   - 若超界或异常，记录并跳过该样本。
3) 默认策略：
   - 支持 4 或 5 个选项；
   - 若 <4 或 >5，跳过并计数（log）。
4) __getitem__ 返回：
   - prompt_text
   - answer_letter（A/B/C/D/E）
   - option_labels（用于 prompt）
   - meta（包含 id、source、原始字段）

## B. Prompt 模板修改（src/data/prompt.py）
1) arc_mcq_v1 模板需要支持可变选项数：
   - Options 段按 option_labels 动态渲染（循环生成 "A. ...", "B. ..." ...）
2) 指令中允许输出集合根据选项变化：
   - 若 4 选项：Return only A/B/C/D
   - 若 5 选项：Return only A/B/C/D/E
   - 代码里自动生成这句，不写死。
3) 依旧保持“Think privately，不输出推理，只输出一个字母”的约束（训练/LoRA 友好）。
4) 提供 format guard：明确“Only output a single capital letter.”，并在 prompt builder 里把 allowed letters 拼成字符串（如 "A, B, C, D, or E"）。

## C. Tokenize/数据批处理修改（collate_fn）
1) batch 内选项数可能不一致（有的 4 有的 5）——这是允许的，因为 prompt_text 是纯文本。
2) 只要每条样本的 prompt_text 构造正确即可；不需要在 batch 中对 option 数做对齐。

## D. 结果与日志
1) meta.json 里记录：
   - num_examples_4opt
   - num_examples_5opt
   - num_skipped_other
2) README 中明确说明支持 A–E，并说明 answerKey 的数字映射规则。

## E.（可选但建议）未来训练对齐的 label 设计提示
虽然当前项目只做采集，但为了后续 SFT/LoRA 训练更顺滑：
- 在 dataset 输出中同时提供 “target_text = answer_letter”；
- 并保留一个函数 build_sft_example(...)，未来训练时直接用（label 对齐只训练单个字母）。

