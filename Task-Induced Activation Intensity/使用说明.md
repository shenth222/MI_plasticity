# 使用说明（中文）

## 项目简介

本项目实现了对 Llama-3.2-1B 模型在 commonsense 任务（CS170k）上的任务诱导信号分析，计算四种 pre-finetuning head scoring 指标。

## 核心功能

✅ **四种评分方法**：
1. Head Output 强度 (L2 norm)
2. Attention Entropy（负熵）
3. Attention to Task-Relevant Tokens（对问题的注意力）
4. 组合分数（基于 rank 的融合）

✅ **Layer-wise Normalization**：z-score 或 percentile

✅ **Top-k 输出**：全局和每层的 Top-k heads

## 快速开始

### 1. 安装依赖

```bash
pip install -r requirements.txt
```

主要依赖：
- torch >= 2.0.0
- transformers >= 4.40.0
- numpy, pandas, scipy
- tqdm, pyyaml

### 2. 准备数据

将您的 CS170k 数据集准备为 JSONL 格式，例如：

```json
{"question": "问题文本", "choices": {"text": ["选项A", "选项B", "选项C", "选项D"], "label": ["A", "B", "C", "D"]}, "answerKey": "A"}
```

或者先使用项目自带的 `example_data.jsonl` 进行测试。

### 3. 配置模型和数据路径

编辑 `configs/default.yaml`：

```yaml
model:
  path: "/你的路径/Llama-3.2-1B"  # 修改这里

data:
  path: "/你的路径/commonsense_170k.jsonl"  # 修改这里
```

### 4. 运行

#### 方式一：使用配置文件（推荐）

```bash
python src/main.py --config configs/default.yaml
```

#### 方式二：使用命令行参数

```bash
python src/main.py \
  --model_path /path/to/Llama-3.2-1B \
  --data_path /path/to/data.jsonl \
  --output_dir outputs/run_001 \
  --max_samples 1024 \
  --batch_size 4 \
  --device cuda:0
```

#### 方式三：使用脚本

```bash
# 1. 编辑 run_example.sh，修改路径
# 2. 运行
chmod +x run_example.sh
./run_example.sh
```

## 主要参数说明

| 参数 | 说明 | 默认值 |
|------|------|--------|
| `--model_path` | Llama-3.2-1B 本地路径 | 必需 |
| `--data_path` | 数据集路径 | 必需 |
| `--output_dir` | 输出目录 | 必需 |
| `--max_samples` | 使用的最大样本数 | 1024 |
| `--batch_size` | 批次大小（根据显存调整） | 4 |
| `--max_length` | 最大序列长度 | 512 |
| `--device` | 设备（cuda:0 或 cpu） | cuda:0 |
| `--dtype` | 数据类型（fp16/bf16/fp32） | fp16 |
| `--seed` | 随机种子 | 42 |
| `--score_query_mode` | last_token 或 all_tokens | last_token |
| `--norm_mode` | zscore 或 percentile | zscore |
| `--lambda_ent` | Entropy 分数权重 | 0.5 |
| `--lambda_task` | Task-align 分数权重 | 1.0 |

## 输出文件

运行完成后，在 `outputs/run_xxx/` 目录会生成：

```
outputs/run_001/
├── config.yaml              # 运行配置备份
├── run.log                  # 运行日志
├── scores_raw.csv          # 原始分数
├── scores_norm.csv         # 归一化分数
├── scores_combined.csv     # 组合分数
├── topk_global.json        # 全局 Top-k heads
└── topk_per_layer.json     # 每层 Top-k heads
```

### 输出文件说明

#### scores_raw.csv
包含每层每个 head 的原始分数：
- `layer`: 层索引（0-based）
- `head`: head 索引（0-based）
- `out_raw`: Head output 强度原始分数
- `ent_raw`: Attention entropy 原始分数（负熵）
- `task_raw`: Task alignment 原始分数

#### scores_norm.csv
经过 layer-wise normalization 的分数（相同列名，后缀为 `_norm`）

#### scores_combined.csv
组合后的最终分数：
- `layer`: 层索引
- `head`: head 索引
- `combined`: 组合分数（用于排序）

#### topk_global.json
全局 Top-k heads：
```json
[
  {"layer": 15, "head": 8, "score": 125.6},
  {"layer": 14, "head": 12, "score": 120.3}
]
```

#### topk_per_layer.json
每层的 Top-k heads：
```json
{
  "0": [{"head": 5, "score": 12.3}, ...],
  "1": [{"head": 3, "score": 15.7}, ...]
}
```

## 常见问题

### Q1: 提示无法捕获 attention probabilities

**A**: 代码已自动处理此问题，会强制使用 `attn_implementation="eager"` 模式。如果仍有问题，请确保 transformers 版本 >= 4.40。

### Q2: CUDA out of memory

**A**: 尝试以下方法：
```bash
# 方法1：减小 batch size
python src/main.py --config configs/default.yaml --batch_size 1

# 方法2：减小序列长度
python src/main.py --config configs/default.yaml --max_length 256

# 方法3：使用 CPU（会很慢）
python src/main.py --config configs/default.yaml --device cpu
```

### Q3: Span 提取失败率高

**A**: 
- 检查 prompt 模板是否包含 "Question:" 和 "Choices:" 标记
- 查看 `run.log` 中的详细错误信息
- 可以修改 `src/data/prompt.py` 中的模板格式

### Q4: 数据集加载失败

**A**:
- 确保数据文件是有效的 JSON 或 JSONL 格式
- 检查字段名称是否匹配（在 `configs/default.yaml` 中配置 `field_mapping`）
- 先用 `example_data.jsonl` 测试是否是数据格式问题

### Q5: 如何适配我的数据集格式？

**A**: 修改 `configs/default.yaml` 中的 `field_mapping`：

```yaml
data:
  field_mapping:
    question: "your_question_field"
    choices_text: "your_choices_field.text"
    choices_label: "your_choices_field.label"
    answer_key: "your_answer_field"
```

## 测试项目

运行以下命令测试项目设置：

```bash
python test_setup.py
```

这会检查：
- ✓ 目录结构完整性
- ✓ 依赖包安装情况
- ✓ 模块导入是否正常
- ✓ 示例数据能否正常加载

## 运行流程

1. **加载配置**：合并配置文件和命令行参数
2. **加载数据集**：解析 JSONL/JSON 数据
3. **加载模型**：加载 Llama-3.2-1B 和 tokenizer
4. **批量推理**：执行 forward pass，捕获 attention 和 hidden states
5. **计算评分**：
   - Head output 强度（L2 norm）
   - Attention entropy（负熵）
   - Task alignment（对问题的注意力）
6. **归一化**：Layer-wise z-score 或 percentile
7. **组合分数**：Rank-based fusion
8. **保存结果**：输出 CSV 和 JSON 文件

## 评分方法详解

### 1. Head Output 强度 (S_out)

衡量每个 attention head 输出的激活强度（L2 范数）。

**公式**: `S_out(h) = ||head_output_h||_2`

### 2. Attention Entropy (S_ent)

衡量注意力分布的集中程度，熵越低说明注意力越集中。

**公式**: `S_ent(h) = -H(attention_probs_h) = -Σ(p*log(p))`

### 3. Task Alignment (S_task)

衡量对任务相关 tokens（question span）的注意力质量。

**公式**: `S_task(h) = Σ(attention_to_question) / span_length`

### 4. 组合分数 (S_combined)

基于 rank 的融合，每种分数先在层内计算 rank，然后加权求和。

**公式**: `S = rank(S_out) + λ1*rank(S_ent) + λ2*rank(S_task)`

## 扩展开发

### 添加新的评分方法

1. 在 `src/scoring/` 下创建新的 Python 文件
2. 实现评分函数，参考现有模块
3. 在 `src/main.py` 中导入并调用

### 支持其他模型

修改 `src/model/load_model.py`，适配不同的模型结构。

### 适配其他任务

修改 `src/data/prompt.py` 中的 prompt 模板。

## 项目结构

```
src/
├── main.py          # 主程序入口
├── args.py          # 参数配置
├── utils/           # 工具函数
├── data/            # 数据加载
├── model/           # 模型推理
└── scoring/         # 评分实现
```

详细说明请查看 `PROJECT_STRUCTURE.md`。

## 引用

如果本项目对您的研究有帮助，欢迎引用相关工作。

## License

MIT License

---

**祝使用顺利！** 如有问题请查看日志文件 `run.log` 或参考 `README.md` 中的详细说明。

