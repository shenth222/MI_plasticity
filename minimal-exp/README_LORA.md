# LoRAå¾®è°ƒå®éªŒæŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨åŸé¡¹ç›®åŸºç¡€ä¸Šä½¿ç”¨**LoRAï¼ˆLow-Rank Adaptationï¼‰**è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œå¹¶ä¸å…¨é‡å¾®è°ƒï¼ˆFFTï¼‰è¿›è¡Œå¯¹æ¯”åˆ†æã€‚

---

## ğŸ“š ç›®å½•

- [LoRAç®€ä»‹](#loraç®€ä»‹)
- [ç¯å¢ƒé…ç½®](#ç¯å¢ƒé…ç½®)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å®éªŒæµç¨‹](#å®éªŒæµç¨‹)
- [å¯¹æ¯”åˆ†æ](#å¯¹æ¯”åˆ†æ)
- [è¾“å‡ºè¯´æ˜](#è¾“å‡ºè¯´æ˜)
- [æŠ€æœ¯ç»†èŠ‚](#æŠ€æœ¯ç»†èŠ‚)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## LoRAç®€ä»‹

**LoRAï¼ˆLow-Rank Adaptationï¼‰**æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼š

- **åŸç†**ï¼šåœ¨é¢„è®­ç»ƒæƒé‡æ—è¾¹æ·»åŠ ä½ç§©çŸ©é˜µ A å’Œ Bï¼Œå†»ç»“åŸå§‹æƒé‡ï¼Œä»…è®­ç»ƒAå’ŒB
- **ä¼˜åŠ¿**ï¼š
  - å¯è®­ç»ƒå‚æ•°é‡å¤§å¹…å‡å°‘ï¼ˆé€šå¸¸<1%ï¼‰
  - è®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼Œæ˜¾å­˜å ç”¨æ›´å°
  - ä¾¿äºå¤šä»»åŠ¡é€‚é…å’Œæ¨¡å‹åˆ†å‘

- **å…¬å¼**ï¼š`h = Wâ‚€x + BAx`ï¼Œå…¶ä¸­ Wâ‚€ æ˜¯å†»ç»“çš„é¢„è®­ç»ƒæƒé‡ï¼ŒB å’Œ A æ˜¯å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µ

---

## ç¯å¢ƒé…ç½®

### ä¾èµ–å®‰è£…

å·²åœ¨ `requirements.txt` ä¸­æ·»åŠ  LoRA æ”¯æŒï¼š

```bash
pip install -r requirements.txt
```

æ–°å¢çš„ä¾èµ–ï¼š
- `peft>=0.7.0` - Hugging Face çš„å‚æ•°é«˜æ•ˆå¾®è°ƒåº“
- `scipy` - ç”¨äºæ•°å€¼è®¡ç®—å’Œç»Ÿè®¡åˆ†æ

---

## å¿«é€Ÿå¼€å§‹

### ä¸€é”®è¿è¡ŒLoRAå®éªŒ

```bash
# 1. è®­ç»ƒLoRAæ¨¡å‹
bash scripts/run_lora.sh 1 RTE 8 16

# 2. æµ‹é‡æŒ‡æ ‡ï¼ˆé‡è¦æ€§ã€å¯å¡‘æ€§ã€æ›´æ–°é‡ï¼‰
bash scripts/measure_lora.sh 1 RTE 8

# 3. å¯è§†åŒ–åˆ†æ
bash scripts/make_plots_lora.sh 1 RTE 8
```

### å¯¹æ¯”FFTå’ŒLoRA

```bash
# å…ˆç¡®ä¿å·²è¿è¡ŒFFTå®éªŒ
bash scripts/run_mnli.sh 1 RTE FFT
bash scripts/measure_mnli.sh 1 RTE FFT
bash scripts/make_plots.sh 1 RTE

# ç„¶åè¿è¡Œå¯¹æ¯”è„šæœ¬
bash scripts/compare_fft_lora.sh 1 RTE 8
```

---

## å®éªŒæµç¨‹

### æ­¥éª¤1ï¼šLoRAè®­ç»ƒ

```bash
bash scripts/run_lora.sh [seed] [task] [lora_r] [lora_alpha]
```

**å‚æ•°è¯´æ˜**ï¼š
- `seed`: éšæœºç§å­ï¼ˆé»˜è®¤ï¼š1ï¼‰
- `task`: GLUEä»»åŠ¡åç§°ï¼ˆé»˜è®¤ï¼šRTEï¼Œå¯é€‰ï¼šMNLI, SST2ç­‰ï¼‰
- `lora_r`: LoRAç§©ï¼ˆé»˜è®¤ï¼š8ï¼Œæ¨èèŒƒå›´ï¼š4-64ï¼‰
- `lora_alpha`: LoRAç¼©æ”¾å› å­ï¼ˆé»˜è®¤ï¼š16ï¼Œé€šå¸¸è®¾ä¸º 2Ã—rï¼‰

**ç¤ºä¾‹**ï¼š

```bash
# ä½¿ç”¨LoRA rank=8è®­ç»ƒRTEä»»åŠ¡
bash scripts/run_lora.sh 1 RTE 8 16

# ä½¿ç”¨LoRA rank=16è®­ç»ƒMNLIä»»åŠ¡
bash scripts/run_lora.sh 1 MNLI 16 32
```

**è¾“å‡º**ï¼š
- `outputs/LoRA/RTE/seed1_r8/ckpt_init/` - åˆå§‹åŸºç¡€æ¨¡å‹ï¼ˆÎ¸0ï¼‰
- `outputs/LoRA/RTE/seed1_r8/ckpt_final/` - LoRAå¾®è°ƒåçš„æ¨¡å‹ï¼ˆÎ¸1ï¼‰
- `outputs/LoRA/RTE/seed1_r8/run_config.json` - è®­ç»ƒé…ç½®

**è®­ç»ƒæ—¶é—´**ï¼šçº¦10-30åˆ†é’Ÿï¼ˆå–å†³äºä»»åŠ¡å’Œæ•°æ®é›†å¤§å°ï¼‰

---

### æ­¥éª¤2ï¼šæµ‹é‡æŒ‡æ ‡

```bash
bash scripts/measure_lora.sh [seed] [task] [lora_r]
```

æ­¤æ­¥éª¤ä¼šä¾æ¬¡æµ‹é‡ï¼š

1. **åˆ›å»ºå›ºå®šè¯„ä¼°å­é›†**ï¼ˆ1024æ¡ï¼Œseed=999ï¼‰
2. **é‡è¦æ€§æµ‹é‡ï¼ˆå¾®è°ƒå‰ï¼‰** - åŸºäºåŸºç¡€æ¨¡å‹Î¸0çš„head ablation
3. **æ¢¯åº¦ä¸Fisher proxyï¼ˆå¾®è°ƒå‰ï¼‰** - æµ‹é‡æ¯ä¸ªheadçš„æ¢¯åº¦å¹…å€¼
4. **æ›´æ–°é‡æµ‹é‡** - è®¡ç®—LoRAæƒé‡åˆå¹¶åçš„å‚æ•°å˜åŒ–
5. **é‡è¦æ€§æµ‹é‡ï¼ˆå¾®è°ƒåï¼‰** - åŸºäºLoRAæ¨¡å‹Î¸1çš„head ablation

**è¾“å‡º**ï¼š
- `eval_subset.json` - å›ºå®šçš„è¯„ä¼°å­é›†ç´¢å¼•
- `importance_pre.jsonl` - å¾®è°ƒå‰é‡è¦æ€§ï¼ˆ144è¡Œï¼Œæ¯ä¸ªheadä¸€è¡Œï¼‰
- `gradfisher_pre.jsonl` - æ¢¯åº¦å’ŒFisher proxy
- `update.jsonl` - æ›´æ–°é‡ï¼ˆç»å¯¹å€¼Uå’Œç›¸å¯¹å€¼Urelï¼‰
- `importance_post.jsonl` - å¾®è°ƒåé‡è¦æ€§

**æµ‹é‡æ—¶é—´**ï¼šçº¦1-2å°æ—¶

---

### æ­¥éª¤3ï¼šå¯è§†åŒ–åˆ†æ

```bash
bash scripts/make_plots_lora.sh [seed] [task] [lora_r]
```

**ç¤ºä¾‹**ï¼š

```bash
bash scripts/make_plots_lora.sh 1 RTE 8
```

**ç”Ÿæˆçš„å›¾è¡¨**ï¼š
- `fig_I_vs_U.png` - é‡è¦æ€§ vs æ›´æ–°é‡æ•£ç‚¹å›¾
- `fig_I_vs_G.png` - é‡è¦æ€§ vs æ¢¯åº¦æ•£ç‚¹å›¾
- `fig_stats.png` - ç»Ÿè®¡æŒ‡æ ‡æŸ±çŠ¶å›¾
- `fig_Ipre_vs_Ipost.png` - å¾®è°ƒå‰åé‡è¦æ€§å¯¹æ¯”
- `fig_Ipost_corrs.png` - å¾®è°ƒåé‡è¦æ€§ä¸å…¶ä»–æŒ‡æ ‡çš„ç›¸å…³æ€§

**ç”Ÿæˆçš„æ•°æ®**ï¼š
- `heads.csv` - æ‰€æœ‰headçš„å®Œæ•´æŒ‡æ ‡è¡¨
- `stats.json` - Spearmanç›¸å…³ç³»æ•°ã€top-Ké‡å åº¦ç­‰ç»Ÿè®¡é‡
- `cases.json` - åä¾‹é›†åˆï¼ˆimportant-but-staticã€plastic-but-unimportantï¼‰

---

## å¯¹æ¯”åˆ†æ

### FFT vs LoRA å¯¹æ¯”

```bash
bash scripts/compare_fft_lora.sh [seed] [task] [lora_r]
```

**å‰ç½®æ¡ä»¶**ï¼š
1. å·²å®ŒæˆFFTå®éªŒï¼ˆè®­ç»ƒ+æµ‹é‡+å¯è§†åŒ–ï¼‰
2. å·²å®ŒæˆLoRAå®éªŒï¼ˆè®­ç»ƒ+æµ‹é‡+å¯è§†åŒ–ï¼‰

**è¾“å‡ºç›®å½•**ï¼š`outputs/COMPARE/[task]/seed[seed]/`

**ç”Ÿæˆçš„å¯¹æ¯”å›¾è¡¨**ï¼š

1. **compare_I_vs_U.png** - å¹¶æ’å¯¹æ¯”FFTå’ŒLoRAçš„é‡è¦æ€§vsæ›´æ–°é‡æ•£ç‚¹å›¾
2. **compare_stats.png** - 4ä¸ªå­å›¾å¯¹æ¯”ï¼š
   - Spearmanç›¸å…³ç³»æ•°
   - Top-Ké‡å åº¦
   - åä¾‹æ•°é‡
   - å¾®è°ƒåé‡è¦æ€§ç›¸å…³æ€§

3. **compare_update_dist.png** - æ›´æ–°é‡åˆ†å¸ƒå¯¹æ¯”
4. **compare_headwise.png** - Head-wiseå¯¹æ¯”ï¼ˆåŒä¸€ä¸ªheadåœ¨FFTå’ŒLoRAä¸­çš„æŒ‡æ ‡ï¼‰

**ç”Ÿæˆçš„å¯¹æ¯”æ•°æ®**ï¼š

- **compare_metrics.json** - ä¸¤ç§æ–¹æ³•çš„å…³é”®æŒ‡æ ‡å¯¹æ¯”
- **compare_summary.txt** - æ–‡å­—æ‘˜è¦æŠ¥å‘Š

**æ‘˜è¦æŠ¥å‘Šç¤ºä¾‹**ï¼š

```
============================================================
å¯¹æ¯”æ‘˜è¦: FFT vs LoRA-r8
============================================================

## 1. Spearmanç›¸å…³ç³»æ•°ï¼ˆé‡è¦æ€§ vs æ›´æ–°é‡ï¼‰
  Ï(I_pre, U):
    FFT: 0.2341
    LoRA-r8: 0.1876
    å·®å¼‚: 0.0465

  Ï(I_pre, Urel):
    FFT: 0.2103
    LoRA-r8: 0.1654
    å·®å¼‚: 0.0449

## 2. Top-20é‡å åº¦
  Top-K overlap (I_pre, U):
    FFT: 0.1500
    LoRA-r8: 0.1250
    å·®å¼‚: 0.0250

## 3. åä¾‹æ•°é‡
  Important-but-static:
    FFT: 8
    LoRA-r8: 10

  Plastic-but-unimportant:
    FFT: 12
    LoRA-r8: 14

## 4. ä¸»è¦å‘ç°
  - LoRA-r8æ˜¾ç¤ºæ›´å¼±çš„ç›¸å…³æ€§ï¼Œæ›´èƒ½è¯´æ˜'é‡è¦æ€§â‰ å¯å¡‘æ€§'
  - LoRA-r8äº§ç”Ÿæ›´å¤šåä¾‹ (24 vs 20)

============================================================
```

---

## è¾“å‡ºè¯´æ˜

### LoRAå®éªŒç›®å½•ç»“æ„

```
outputs/LoRA/RTE/seed1_r8/
â”œâ”€â”€ ckpt_init/              # Î¸0: åˆå§‹åŸºç¡€æ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ckpt_final/             # Î¸1: LoRAé€‚é…å™¨æƒé‡
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.bin
â”‚   â””â”€â”€ ...
â”œâ”€â”€ run_config.json         # è®­ç»ƒé…ç½®
â”œâ”€â”€ eval_subset.json        # å›ºå®šè¯„ä¼°å­é›†
â”œâ”€â”€ importance_pre.jsonl    # å¾®è°ƒå‰é‡è¦æ€§
â”œâ”€â”€ gradfisher_pre.jsonl    # æ¢¯åº¦ä¸Fisher
â”œâ”€â”€ update.jsonl            # æ›´æ–°é‡
â”œâ”€â”€ importance_post.jsonl   # å¾®è°ƒåé‡è¦æ€§
â”œâ”€â”€ heads.csv               # æ±‡æ€»è¡¨
â”œâ”€â”€ stats.json              # ç»Ÿè®¡é‡
â”œâ”€â”€ cases.json              # åä¾‹é›†åˆ
â””â”€â”€ fig_*.png               # å¯è§†åŒ–å›¾è¡¨
```

### å¯¹æ¯”å®éªŒç›®å½•ç»“æ„

```
outputs/COMPARE/RTE/seed1/
â”œâ”€â”€ compare_I_vs_U.png      # æ•£ç‚¹å›¾å¯¹æ¯”
â”œâ”€â”€ compare_stats.png       # ç»Ÿè®¡æŒ‡æ ‡å¯¹æ¯”
â”œâ”€â”€ compare_update_dist.png # æ›´æ–°é‡åˆ†å¸ƒå¯¹æ¯”
â”œâ”€â”€ compare_headwise.png    # Head-wiseå¯¹æ¯”
â”œâ”€â”€ compare_metrics.json    # å¯¹æ¯”æŒ‡æ ‡
â””â”€â”€ compare_summary.txt     # å¯¹æ¯”æ‘˜è¦
```

---

## æŠ€æœ¯ç»†èŠ‚

### LoRAé…ç½®

åœ¨ `finetune_glue_lora.py` ä¸­é…ç½®ï¼š

```python
lora_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    r=8,                    # ç§©ï¼ˆrankï¼‰
    lora_alpha=16,          # ç¼©æ”¾å› å­
    lora_dropout=0.1,       # dropoutç‡
    target_modules=[        # ç›®æ ‡æ¨¡å—
        "query_proj",       # QæŠ•å½±
        "key_proj",         # KæŠ•å½±
        "value_proj",       # VæŠ•å½±
        "dense"             # è¾“å‡ºæŠ•å½±ï¼ˆOï¼‰
    ],
    bias="none",            # ä¸è®­ç»ƒbias
    inference_mode=False,   # è®­ç»ƒæ¨¡å¼
)
```

### LoRAæ›´æ–°é‡è®¡ç®—

LoRAçš„æ›´æ–°é‡æµ‹é‡éœ€è¦ç‰¹æ®Šå¤„ç†ï¼š

1. **åŠ è½½LoRAæ¨¡å‹**ï¼šåŸºç¡€æ¨¡å‹ + LoRAé€‚é…å™¨
2. **åˆå¹¶æƒé‡**ï¼š`W_new = W_base + B @ A * (alpha/r)`
3. **è®¡ç®—æ›´æ–°é‡**ï¼šä¸FFTç›¸åŒçš„æ–¹å¼è®¡ç®— `||W_new - W_base||`

**å…³é”®ä»£ç **ï¼ˆ`update_magnitude_lora.py`ï¼‰ï¼š

```python
# åŠ è½½LoRAæ¨¡å‹å¹¶åˆå¹¶æƒé‡
base_model = AutoModelForSequenceClassification.from_pretrained(ckpt_init)
lora_model = PeftModel.from_pretrained(base_model, ckpt_final)
merged_model = lora_model.merge_and_unload()

# è®¡ç®—head-levelæ›´æ–°é‡
for layer, head in enumerate_heads:
    delta_q = merged_Wq[head_slice] - base_Wq[head_slice]
    delta_k = merged_Wk[head_slice] - base_Wk[head_slice]
    delta_v = merged_Wv[head_slice] - base_Wv[head_slice]
    delta_o = merged_Wo[:, head_slice] - base_Wo[:, head_slice]
    
    U = sqrt(||delta_q||Â² + ||delta_k||Â² + ||delta_v||Â² + ||delta_o||Â²)
```

### LoRAé‡è¦æ€§æµ‹é‡

é‡è¦æ€§æµ‹é‡ä½¿ç”¨**head gating**æŠ€æœ¯ï¼Œä¸FFTç›¸åŒï¼š

1. **åˆå¹¶LoRAæƒé‡**ä»¥è·å¾—å®Œæ•´æ¨¡å‹
2. **æ³¨å…¥head gates**åˆ°attentionå±‚
3. **é€ä¸ªablationæ¯ä¸ªhead**ï¼Œæµ‹é‡losså˜åŒ–
4. **é‡è¦æ€§** = loss_ablate - loss_base

**æ³¨æ„**ï¼šç”±äºhead gatingæ˜¯åœ¨attentionè¾“å‡ºå±‚é¢æ“ä½œçš„ï¼Œä¸æƒé‡æ˜¯FFTè¿˜æ˜¯LoRAæ— å…³ï¼Œå› æ­¤å¯ä»¥ç›´æ¥åº”ç”¨ã€‚

---

## å¸¸è§é—®é¢˜

### 1. LoRAå’ŒFFTçš„åŒºåˆ«

**è®­ç»ƒè¿‡ç¨‹**ï¼š
- **FFT**ï¼šæ›´æ–°æ‰€æœ‰å‚æ•°ï¼ˆ~125Må‚æ•°ï¼‰
- **LoRA**ï¼šåªæ›´æ–°ä½ç§©çŸ©é˜µï¼ˆ~0.3Må‚æ•°ï¼Œr=8æ—¶ï¼‰

**æ›´æ–°é‡**ï¼š
- **FFT**ï¼šç›´æ¥ä¿®æ”¹åŸå§‹æƒé‡
- **LoRA**ï¼šé€šè¿‡ `B @ A` æ·»åŠ å¢é‡æ›´æ–°

**é€‚ç”¨åœºæ™¯**ï¼š
- **FFT**ï¼šè¿½æ±‚æœ€ä½³æ€§èƒ½ï¼Œèµ„æºå……è¶³
- **LoRA**ï¼šå‚æ•°é«˜æ•ˆï¼Œå¿«é€Ÿé€‚é…ï¼Œå¤šä»»åŠ¡åœºæ™¯

### 2. å¦‚ä½•é€‰æ‹©LoRA rank (r)?

**å»ºè®®**ï¼š
- **r=4**: æè‡´å‚æ•°æ•ˆç‡ï¼Œæ€§èƒ½å¯èƒ½å—é™
- **r=8**: æ¨èé»˜è®¤å€¼ï¼Œå¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡
- **r=16**: æ›´å¥½çš„æ€§èƒ½ï¼Œæ¥è¿‘å…¨é‡å¾®è°ƒ
- **r=32-64**: æ€§èƒ½æ¥è¿‘FFTï¼Œä½†å‚æ•°é‡å¢åŠ 

**å®éªŒ**ï¼šå¯ä»¥è¿è¡Œå¤šä¸ªrankè¿›è¡Œå¯¹æ¯”ï¼š

```bash
for r in 4 8 16 32; do
    bash scripts/run_lora.sh 1 RTE $r $((r*2))
    bash scripts/measure_lora.sh 1 RTE $r
    bash scripts/make_plots.sh 1 RTE LoRA_r${r}
done
```

### 3. LoRAæ¨¡å‹åŠ è½½å¤±è´¥

**ç—‡çŠ¶**ï¼š`RuntimeError: Error(s) in loading state_dict`

**åŸå› **ï¼šLoRAæƒé‡å’ŒåŸºç¡€æ¨¡å‹ä¸åŒ¹é…

**è§£å†³**ï¼š
1. ç¡®ä¿ `ckpt_init` å’Œ `ckpt_final` æ¥è‡ªåŒä¸€æ¬¡è®­ç»ƒ
2. æ£€æŸ¥ `adapter_config.json` ä¸­çš„ `base_model_name_or_path`
3. é‡æ–°è¿è¡Œè®­ç»ƒè„šæœ¬

### 4. å†…å­˜ä¸è¶³ (OOM)

**LoRAä¼˜åŠ¿**ï¼šæ˜¾å­˜å ç”¨æ¯”FFTå°‘30-50%

**å¦‚æœä»ç„¶OOM**ï¼š
- å‡å° `batch_size`ï¼ˆé»˜è®¤128ï¼Œå¯æ”¹ä¸º64æˆ–32ï¼‰
- å‡å° `max_len`ï¼ˆé»˜è®¤256ï¼Œå¯æ”¹ä¸º128ï¼‰
- ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼š`--gradient_accumulation_steps 2`

### 5. åˆå¹¶æƒé‡åç²¾åº¦ä¸‹é™

**ç—‡çŠ¶**ï¼š`merge_and_unload()` åç²¾åº¦ä¸åŸLoRAæ¨¡å‹ä¸ä¸€è‡´

**åŸå› **ï¼šæ•°å€¼ç²¾åº¦é—®é¢˜æˆ–å½’ä¸€åŒ–å·®å¼‚

**è§£å†³**ï¼š
1. ä½¿ç”¨ `fp32` è¿›è¡Œåˆå¹¶ï¼š`model = model.float().merge_and_unload()`
2. å¦‚æœä»…ç”¨äºablationï¼Œç²¾åº¦å·®å¼‚<0.1%æ˜¯å¯æ¥å—çš„

### 6. å¦‚ä½•åœ¨æ¨ç†ä¸­ä½¿ç”¨LoRAæ¨¡å‹ï¼Ÿ

**ä¸åˆå¹¶æƒé‡**ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰ï¼š

```python
from peft import PeftModel
base_model = AutoModelForSequenceClassification.from_pretrained("path/to/ckpt_init")
lora_model = PeftModel.from_pretrained(base_model, "path/to/ckpt_final")
lora_model.eval()
# ç›´æ¥æ¨ç†
outputs = lora_model(**inputs)
```

**åˆå¹¶æƒé‡**ï¼ˆå•ä¸€æ¨¡å‹éƒ¨ç½²ï¼‰ï¼š

```python
merged_model = lora_model.merge_and_unload()
merged_model.save_pretrained("path/to/merged_model")
# åç»­å¯ç›´æ¥åŠ è½½merged_model
```

---

## è¿›é˜¶å®éªŒ

### å¤šç§å­å¯¹æ¯”

```bash
#!/bin/bash
# è¿è¡Œå¤šä¸ªç§å­çš„FFTå’ŒLoRAå®éªŒ

for seed in 1 2 3 4 5; do
    echo "=== Seed ${seed} ==="
    
    # FFT
    bash scripts/run_mnli.sh ${seed} RTE FFT
    bash scripts/measure_mnli.sh ${seed} RTE FFT
    bash scripts/make_plots.sh ${seed} RTE
    
    # LoRA
    bash scripts/run_lora.sh ${seed} RTE 8 16
    bash scripts/measure_lora.sh ${seed} RTE 8
    bash scripts/make_plots_lora.sh ${seed} RTE 8
    
    # å¯¹æ¯”
    bash scripts/compare_fft_lora.sh ${seed} RTE 8
done

# æ±‡æ€»å¤šç§å­ç»“æœ
python -m src.analysis.aggregate_seeds \
    --method FFT LoRA_r8 \
    --task RTE \
    --seeds 1 2 3 4 5 \
    --out_dir outputs/MULTI_SEED/
```

### ä¸åŒrankå¯¹æ¯”

```bash
#!/bin/bash
# å¯¹æ¯”ä¸åŒLoRA rankçš„æ•ˆæœ

TASK="RTE"
SEED=1

for r in 4 8 16 32; do
    alpha=$((r * 2))
    
    echo "=== LoRA rank=${r} ==="
    bash scripts/run_lora.sh ${SEED} ${TASK} ${r} ${alpha}
    bash scripts/measure_lora.sh ${SEED} ${TASK} ${r}
    bash scripts/make_plots_lora.sh ${SEED} ${TASK} ${r}
done

# å¯¹æ¯”ä¸åŒrank
python -m src.analysis.compare_lora_ranks \
    --task ${TASK} \
    --seed ${SEED} \
    --ranks 4 8 16 32 \
    --out_dir outputs/RANK_COMPARISON/
```

---

## ç›¸å…³è®ºæ–‡

1. **LoRA: Low-Rank Adaptation of Large Language Models**
   - Hu et al., ICLR 2022
   - https://arxiv.org/abs/2106.09685

2. **Parameter-Efficient Transfer Learning for NLP**
   - Houlsby et al., ICML 2019
   - Adapteræ–¹æ³•çš„åŸå§‹è®ºæ–‡

3. **The Power of Scale for Parameter-Efficient Prompt Tuning**
   - Lester et al., EMNLP 2021
   - Prompt tuningæ–¹æ³•

---

## æ€»ç»“

æœ¬LoRAæ‰©å±•æä¾›äº†ï¼š

âœ… **å®Œæ•´çš„LoRAè®­ç»ƒpipeline**  
âœ… **ä¸FFTç­‰ä»·çš„æµ‹é‡æ–¹æ³•**ï¼ˆé‡è¦æ€§ã€å¯å¡‘æ€§ã€æ›´æ–°é‡ï¼‰  
âœ… **è¯¦ç»†çš„å¯¹æ¯”åˆ†æå·¥å…·**ï¼ˆå¯è§†åŒ–+ç»Ÿè®¡ï¼‰  
âœ… **çµæ´»çš„é…ç½®é€‰é¡¹**ï¼ˆrankã€alphaã€target_modulesï¼‰  
âœ… **æ¸…æ™°çš„æ–‡æ¡£å’Œç¤ºä¾‹**  

é€šè¿‡å¯¹æ¯”FFTå’ŒLoRAï¼Œæ‚¨å¯ä»¥ï¼š
- éªŒè¯"é‡è¦æ€§â‰ å¯å¡‘æ€§"è¿™ä¸€å‘ç°åœ¨ä¸åŒå¾®è°ƒæ–¹æ³•ä¸‹çš„æ™®éæ€§
- ç ”ç©¶å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¯¹æ¨¡å‹å¯å¡‘æ€§çš„å½±å“
- æ¢ç´¢ä¸åŒLoRAé…ç½®å¯¹headæ›´æ–°æ¨¡å¼çš„å½±å“

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸ‰

---

**ç”Ÿæˆæ—¶é—´**ï¼š2026-01-27  
**ä½œè€…**ï¼šAI Assistant (Claude Sonnet 4.5)  
**é¡¹ç›®è·¯å¾„**ï¼š`/data1/shenth/work/MI_plasticity/minimal-exp`
